{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24f2bd0-d49c-491a-93d0-16e7812ac307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import laion_clap\n",
    "import numpy as np\n",
    "import librosa\n",
    "from msclap import CLAP\n",
    "#from transformers import ClapModel, ClapProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8489e25a-c438-4bb6-b928-d41118c68a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b55c86-aacd-43d9-bc98-9cc01376b5ab",
   "metadata": {},
   "source": [
    "Load **CLIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4437e6-8bf4-48ad-9065-01e56747775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "027f713b-0273-4cfa-b91a-122c30fc1f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [03:19<00:00, 1.77MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model1, preprocess1 = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea79dfe-6d31-4a91-bd6c-8958c78d1f40",
   "metadata": {},
   "source": [
    "Load **Laion_CLAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5940d57e-8582-450c-a093-b1b67b091c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization\n",
    "def int16_to_float32(x):\n",
    "    return (x / 32767.0).astype(np.float32)\n",
    "\n",
    "\n",
    "def float32_to_int16(x):\n",
    "    x = np.clip(x, a_min=-1., a_max=1.)\n",
    "    \n",
    "#model2 = laion_clap.CLAP_Module(enable_fusion=False)\n",
    "#model2.load_ckpt() # download the default pretrained checkpoint.\n",
    "model2 = laion_clap.CLAP_Module(enable_fusion=False, amodel= 'HTSAT-base')\n",
    "model2.load_ckpt('../Models/Laion_CLAP/music_audioset_epoch_15_esc_90.14.patched.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ffe648-a31e-4451-ab2c-8cb4ddb1995b",
   "metadata": {},
   "source": [
    "load **Microsoft_CLAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5b7ee-ec95-4cda-9ba9-56aff6c671d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = CLAP(version = '2023', use_cuda=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9b012-4d12-41dc-a068-c3051818a548",
   "metadata": {},
   "source": [
    "**CLIP** textdata-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6635e645-fa9c-4115-81ef-db1fb3640a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_add_embedding(csv_path, column_name, new_column_name):\n",
    "    try:\n",
    "        # CSVファイルの読み込み\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # 新しい列 \"caption_clip\" を追加\n",
    "        df[new_column_name] = None\n",
    "\n",
    "        # 指定された列のデータをリストとして取得\n",
    "        text_data_list = df[column_name].tolist()\n",
    "\n",
    "        # ERROR回数カウント変数\n",
    "        e_num = 0\n",
    "\n",
    "        # テキストデータを順に処理して \"clip_text_embed\" に格納\n",
    "        for i, text_data in enumerate(text_data_list):\n",
    "            try:\n",
    "                with torch.no_grad():\n",
    "                    token = clip.tokenize(text_data).to(device)\n",
    "                    embedding_tensor = model1.encode_text(token)\n",
    "\n",
    "                # \"clip_text_embed\" 列にテンソルを格納\n",
    "                df.at[i, new_column_name] = embedding_tensor\n",
    "\n",
    "                # CUDAメモリを解放\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                # エラーが発生した場合、例外メッセージを格納\n",
    "                df.at[i, new_column_name] = \"ERROR\"\n",
    "                print(str(e))\n",
    "                e_num = e_num +1\n",
    "\n",
    "        # 結果を表示\n",
    "        print(df)\n",
    "        print(f\"ERROR数：{e_num}\")\n",
    "\n",
    "        # CSVファイルに結果を保存\n",
    "        df.to_csv(\"../Datasets/archive/musiccaps-processed.csv\", index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "\n",
    "# CSVファイル、抽出列を指定\n",
    "csv_file_path = \"../Datasets/archive/musiccaps-public.csv\"\n",
    "\n",
    "target_column_name = \"aspect_list\" # clip_text_embed用\n",
    "#target_column_name = \"caption\" # caption_clip用\n",
    "create_column_name = \"clip_text_embed\"\n",
    "\n",
    "process_and_add_embedding(csv_file_path, target_column_name, create_column_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932b219f-96cb-4c29-a830-cd944a5a2ab8",
   "metadata": {},
   "source": [
    "**CLAP** textdata-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "412462a2-b55d-4586-b323-9250e7450dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ytid  start_s  end_s  \\\n",
      "0     -0Gj8-vB1q4       30     40   \n",
      "1     -0SdAVK79lg       30     40   \n",
      "2     -0vPFx-wRRI       30     40   \n",
      "3     -0xzrMun0Rs       30     40   \n",
      "4     -1LrH01Ei1w       30     40   \n",
      "...           ...      ...    ...   \n",
      "5516  zw5dkiklbhE       15     25   \n",
      "5517  zwfo7wnXdjs       30     40   \n",
      "5518  zx_vcwOsDO4       50     60   \n",
      "5519  zyXa2tdBTGc       30     40   \n",
      "5520  zzNdwF40ID8       70     80   \n",
      "\n",
      "                               audioset_positive_labels  \\\n",
      "0                          /m/0140xf,/m/02cjck,/m/04rlf   \n",
      "1     /m/0155w,/m/01lyv,/m/0342h,/m/042v_gx,/m/04rlf...   \n",
      "2                                   /m/025_jnm,/m/04rlf   \n",
      "3                                    /m/01g90h,/m/04rlf   \n",
      "4                                   /m/02p0sh1,/m/04rlf   \n",
      "...                                                 ...   \n",
      "5516                                /m/01sm1g,/m/0l14md   \n",
      "5517                      /m/02p0sh1,/m/04rlf,/m/06j64v   \n",
      "5518  /m/01glhc,/m/02sgy,/m/0342h,/m/03lty,/m/04rlf,...   \n",
      "5519                                /m/04rlf,/t/dd00034   \n",
      "5520                                  /m/04rlf,/m/0790c   \n",
      "\n",
      "                                            aspect_list  \\\n",
      "0     ['low quality', 'sustained strings melody', 's...   \n",
      "1     ['guitar song', 'piano backing', 'simple percu...   \n",
      "2     ['amateur recording', 'finger snipping', 'male...   \n",
      "3     ['backing track', 'jazzy', 'digital drums', 'p...   \n",
      "4     ['rubab instrument', 'repetitive melody on dif...   \n",
      "...                                                 ...   \n",
      "5516  ['amateur recording', 'percussion', 'wooden bo...   \n",
      "5517  ['instrumental music', 'arabic music', 'genera...   \n",
      "5518  ['instrumental', 'no voice', 'electric guitar'...   \n",
      "5519  ['instrumental music', 'gospel music', 'strong...   \n",
      "5520  ['glitch', 'noise', 'instrumental', 'electroni...   \n",
      "\n",
      "                                                caption  author_id  \\\n",
      "0     The low quality recording features a ballad so...          4   \n",
      "1     This song features an electric guitar as the m...          0   \n",
      "2     a male voice is singing a melody with changing...          6   \n",
      "3     This song contains digital drums playing a sim...          6   \n",
      "4     This song features a rubber instrument being p...          0   \n",
      "...                                                 ...        ...   \n",
      "5516  This audio contains someone playing a wooden b...          6   \n",
      "5517  The song is an instrumental. The song is mediu...          1   \n",
      "5518  The rock music is purely instrumental and feat...          2   \n",
      "5519  The song is an instrumental. The song is slow ...          1   \n",
      "5520  This is a glitch music piece. There is a synth...          9   \n",
      "\n",
      "      is_balanced_subset  is_audioset_eval  \\\n",
      "0                  False              True   \n",
      "1                  False             False   \n",
      "2                  False              True   \n",
      "3                  False              True   \n",
      "4                  False             False   \n",
      "...                  ...               ...   \n",
      "5516               False             False   \n",
      "5517                True              True   \n",
      "5518                True              True   \n",
      "5519               False             False   \n",
      "5520                True              True   \n",
      "\n",
      "                                        clip_text_embed  \\\n",
      "0     tensor([[ 4.7363e-02, -9.1614e-02,  3.0347e-01...   \n",
      "1     tensor([[ 1.9202e-01, -3.4912e-02,  1.1707e-01...   \n",
      "2     tensor([[-1.9360e-01, -2.8473e-02,  1.0870e-01...   \n",
      "3     tensor([[ 1.2183e-01,  2.7539e-01,  8.6365e-03...   \n",
      "4     tensor([[-1.1261e-01, -1.0876e-01, -2.4182e-01...   \n",
      "...                                                 ...   \n",
      "5516  tensor([[-2.6428e-02, -1.6687e-01,  3.6670e-01...   \n",
      "5517                                              ERROR   \n",
      "5518  tensor([[-8.0444e-02, -2.0679e-01, -1.2427e-01...   \n",
      "5519                                              ERROR   \n",
      "5520  tensor([[ 2.0206e-05,  4.7998e-01, -2.1802e-01...   \n",
      "\n",
      "                                        clap_text_embed  \n",
      "0     [[tensor(-0.0357, device='cuda:0'), tensor(0.0...  \n",
      "1     [[tensor(-0.0243, device='cuda:0'), tensor(0.0...  \n",
      "2     [[tensor(-0.0574, device='cuda:0'), tensor(0.0...  \n",
      "3     [[tensor(-0.0144, device='cuda:0'), tensor(0.0...  \n",
      "4     [[tensor(-0.1085, device='cuda:0'), tensor(0.0...  \n",
      "...                                                 ...  \n",
      "5516  [[tensor(0.0028, device='cuda:0'), tensor(0.05...  \n",
      "5517  [[tensor(-0.0848, device='cuda:0'), tensor(0.0...  \n",
      "5518  [[tensor(-0.0467, device='cuda:0'), tensor(0.0...  \n",
      "5519  [[tensor(-0.0113, device='cuda:0'), tensor(0.0...  \n",
      "5520  [[tensor(-0.0889, device='cuda:0'), tensor(0.0...  \n",
      "\n",
      "[5521 rows x 11 columns]\n",
      "ERROR数：0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clap_process_and_add_embedding(csv_path, column_name):\n",
    "    try:\n",
    "        # CSVファイルの読み込み\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # 新しい列 \"clap_text_embed\" を追加\n",
    "        df[\"clap_text_embed\"] = None\n",
    "\n",
    "        # 指定された列のデータをリストとして取得\n",
    "        text_data_list = df[column_name].tolist()\n",
    "\n",
    "        # ERROR回数カウント変数\n",
    "        e_num = 0\n",
    "\n",
    "        # テキストデータを順に処理して \"clap_text_embed\" に格納\n",
    "        for j, text_data in enumerate(text_data_list):\n",
    "            try:\n",
    "                # text_dataを2要素の配列に変換\n",
    "                text_data_array = [text_data, \"\"]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # model2.get_text_embeddingの前にtext_dataを配列に変換\n",
    "                    text_embed = model2.get_text_embedding(text_data_array, use_tensor=True).to(device)\n",
    "\n",
    "                    # [2, 512]のTensorから第一要素を取り出し、[1, 512]のTensorにする\n",
    "                    text_embed = text_embed[0].unsqueeze(0)\n",
    "\n",
    "                # \"clap_text_embed\" 列にテンソルを格納\n",
    "                df.at[j, \"clap_text_embed\"] = text_embed\n",
    "\n",
    "                # CUDAメモリを解放\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                # エラーが発生した場合、例外メッセージを格納\n",
    "                df.at[j, \"clap_text_embed\"] = \"ERROR\"\n",
    "                print(str(e))\n",
    "                e_num = e_num +1\n",
    "\n",
    "        # 結果を表示\n",
    "        print(df)\n",
    "        print(f\"ERROR数：{e_num}\")\n",
    "\n",
    "        # CSVファイルに結果を保存\n",
    "        df.to_csv(\"../Datasets/archive/musiccaps-processed.csv\", index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "\n",
    "# CSVファイル、抽出列を指定\n",
    "csv_file_path = \"../Datasets/archive/musiccaps-processed.csv\"\n",
    "target_column_name = \"aspect_list\"\n",
    "#target_column_name = \"caption\" # caption_clip用\n",
    "\n",
    "clap_process_and_add_embedding(csv_file_path, target_column_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ffc04d-3203-43d3-ba89-184421ac6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "df = pd.read_csv(csv_file_path)\n",
    "text_data_list = df[\"clap_text_embed\"].tolist()\n",
    "tensor_embed = torch.tensor(eval(text_data_list[0]))\n",
    "print(tensor_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab202f8-0676-4237-845f-1051b958a1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to recreate Tensor from CSV`s string object\n",
    "import pandas as pd\n",
    "import torch\n",
    "import ast\n",
    "\n",
    "def load_csv_and_convert_to_tensor(csv_path):\n",
    "    try:\n",
    "        # CSVファイルの読み込み\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # \"clap_text_embed\" 列のデータをテンソル形式に変換して格納するリスト\n",
    "        tensor_list = []\n",
    "\n",
    "        # \"clap_text_embed\" 列のデータを順に処理してテンソルに変換\n",
    "        for text_embed_str in df[\"clap_text_embed\"]:\n",
    "            try:\n",
    "                # 文字列形式のテンソルをリストに変換\n",
    "                tensor_list_str = ast.literal_eval(text_embed_str)\n",
    "\n",
    "                # リストをPyTorchのテンソルに変換\n",
    "                text_embed_tensor = torch.tensor(tensor_list_str)\n",
    "\n",
    "                # テンソルをリストに追加\n",
    "                tensor_list.append(text_embed_tensor)\n",
    "\n",
    "            except Exception as e:\n",
    "                # エラーが発生した場合、Noneを追加しておく\n",
    "                tensor_list.append(None)\n",
    "                print(str(e))\n",
    "\n",
    "        # 結果を表示\n",
    "        for tensor in tensor_list:\n",
    "            print(tensor)\n",
    "\n",
    "        # テンソル形式のリストを返す\n",
    "        return tensor_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "        return None\n",
    "\n",
    "# CSVファイルのパスを指定\n",
    "csv_file_path = \"../Datasets/archive/musiccaps-processed.csv\"\n",
    "\n",
    "# CSVファイルからテンソル形式のリストを読み込む\n",
    "tensor_list = load_csv_and_convert_to_tensor(csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefc103-1177-4676-b8f0-c5fc586635d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data_list = df[\"clip_text_embed\"].tolist()\n",
    "print(text_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ce432-e60d-4a8a-a891-8d455b7dbb0a",
   "metadata": {},
   "source": [
    "**CLAP** textdata-processing ->save to .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6881b4c-cc3f-4710-9ba1-247961698522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def clip_process_and_add_embedding(csv_path, column_name, save_path):\n",
    "    try:\n",
    "        # CSVファイルの読み込み\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # 新しい列 \"clap_text_embed\" を追加\n",
    "        df[\"clip_text_embed\"] = None\n",
    "\n",
    "        # 指定された列のデータをリストとして取得\n",
    "        text_data_list = df[column_name].tolist()\n",
    "\n",
    "        # NumPy配列で結果を保存するためのリスト\n",
    "        embedding_array_list = []\n",
    "\n",
    "        # テキストデータを順に処理して \"clap_text_embed\" に格納\n",
    "        for text_data in text_data_list:\n",
    "            try:\n",
    "                # text_dataを2要素の配列に変換\n",
    "                text_data_array = [text_data, \"\"]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # model2.get_text_embeddingの前にtext_dataを配列に変換\n",
    "                    text_embed = model2.get_text_embedding(text_data_array, use_tensor=True).to(device)\n",
    "\n",
    "                    # [2, 512]のTensorから第一要素を取り出し、[1, 512]のTensorにする\n",
    "                    text_embed = text_embed[0].unsqueeze(0)\n",
    "\n",
    "                    # CUDAデバイス上のテンソルをCPU上のNumPy配列に変換\n",
    "                    text_embed_np = text_embed.cpu().numpy()\n",
    "\n",
    "                # テンソルをNumPy配列に変換してリストに追加\n",
    "                embedding_array_list.append(text_embed_np)\n",
    "\n",
    "            except Exception as e:\n",
    "                # エラーが発生した場合、エラー内容を表示して保存する\n",
    "                error_message = str(e)\n",
    "                embedding_array_list.append(np.array([float('nan')] * 512))  # エラーの場合はNaNで埋める\n",
    "                print(f\"エラーが発生しました: {error_message}\")\n",
    "\n",
    "        # NumPy配列のリストを結合して、2次元のNumPy配列に変換\n",
    "        embedding_array = np.vstack(embedding_array_list)\n",
    "\n",
    "        # NumPy配列を保存\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(embedding_array, f)\n",
    "\n",
    "        # 結果を表示\n",
    "        print(\"結果が保存されました。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "\n",
    "# CSVファイル、抽出列、保存パスを指定\n",
    "csv_file_path = \"../Datasets/archive/musiccaps-processed.csv\"\n",
    "target_column_name = \"aspect_list\"\n",
    "numpy_save_path = \"../Datasets/archive/clip_embeds.npy\"\n",
    "\n",
    "clip_process_and_add_embedding(csv_file_path, target_column_name, numpy_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d72ef6-fd38-4c47-a80d-edd7df3befaa",
   "metadata": {},
   "source": [
    "**CLAP** textdata-processing ->save to .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab8175-dafa-45a5-8a6f-e59247a9ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def clap_process_and_add_embedding(csv_path, column_name, save_path):\n",
    "    try:\n",
    "        # CSVファイルの読み込み\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # 新しい列 \"clap_text_embed\" を追加\n",
    "        df[\"clap_text_embed\"] = None\n",
    "\n",
    "        # 指定された列のデータをリストとして取得\n",
    "        text_data_list = df[column_name].tolist()\n",
    "\n",
    "        # NumPy配列で結果を保存するためのリスト\n",
    "        embedding_array_list = []\n",
    "\n",
    "        # テキストデータを順に処理して \"clap_text_embed\" に格納\n",
    "        for text_data in text_data_list:\n",
    "            try:\n",
    "                # text_dataを2要素の配列に変換\n",
    "                text_data_array = [text_data, \"\"]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # model2.get_text_embeddingの前にtext_dataを配列に変換\n",
    "                    text_embed = model2.get_text_embedding(text_data_array, use_tensor=True).to(device)\n",
    "\n",
    "                    # [2, 512]のTensorから第一要素を取り出し、[1, 512]のTensorにする\n",
    "                    text_embed = text_embed[0].unsqueeze(0)\n",
    "\n",
    "                    # CUDAデバイス上のテンソルをCPU上のNumPy配列に変換\n",
    "                    text_embed_np = text_embed.cpu().numpy()\n",
    "\n",
    "                # テンソルをNumPy配列に変換してリストに追加\n",
    "                embedding_array_list.append(text_embed_np)\n",
    "\n",
    "            except Exception as e:\n",
    "                # エラーが発生した場合、エラー内容を表示して保存する\n",
    "                error_message = str(e)\n",
    "                embedding_array_list.append(np.array([float('nan')] * 512))  # エラーの場合はNaNで埋める\n",
    "                print(f\"エラーが発生しました: {error_message}\")\n",
    "\n",
    "        # NumPy配列のリストを結合して、2次元のNumPy配列に変換\n",
    "        embedding_array = np.vstack(embedding_array_list)\n",
    "\n",
    "        # NumPy配列を保存\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(embedding_array, f)\n",
    "\n",
    "        # 結果を表示\n",
    "        print(\"結果が保存されました。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "\n",
    "# CSVファイル、抽出列、保存パスを指定\n",
    "csv_file_path = \"../Datasets/archive/musiccaps-processed.csv\"\n",
    "target_column_name = \"aspect_list\"\n",
    "numpy_save_path = \"../Datasets/archive/clap_embeds.npy\"\n",
    "\n",
    "clap_process_and_add_embedding(csv_file_path, target_column_name, numpy_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b6ae20-d8bc-4e55-8479-10314676d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# npyファイルからNumPy配列を読み込む\n",
    "numpy_array = np.load(numpy_save_path, allow_pickle=True)\n",
    "\n",
    "# NumPy配列をPyTorchのテンソルに変換する\n",
    "tensor = torch.tensor(numpy_array)\n",
    "\n",
    "# 最初の要素を表示する\n",
    "print(\"最初の要素:\", tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7bd5ce-5738-4c40-89d0-c1978af4bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor[0].shape)\n",
    "print(tensor[0].unsqueeze(0))\n",
    "print(tensor[0].unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3849c36-7685-494c-bbf1-7ac0252bffdc",
   "metadata": {},
   "source": [
    "**CLIP**&**CLAP** textdata-processing ->save to .npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607d1fdb-2462-4804-b1b0-953d3df85cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def clap_process_and_add_embedding(csv_path, column_name, save_path):\n",
    "    try:\n",
    "        # CSVファイルの読み込み\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # 新しい列 \"clap_text_embed_A\" と \"clap_text_embed_B\" を追加\n",
    "        df[\"clip_text_embed\"] = None\n",
    "        df[\"clap_text_embed\"] = None\n",
    "\n",
    "        # 指定された列のデータをリストとして取得\n",
    "        text_data_list = df[column_name].tolist()\n",
    "\n",
    "        # NumPy配列で結果を保存するためのリスト（AとBの処理結果用）\n",
    "        embedding_array_list_clip = []\n",
    "        embedding_array_list_clap = []\n",
    "\n",
    "        # テキストデータを順に処理して \"clap_text_embed\" に格納\n",
    "        for text_data in text_data_list:\n",
    "            try:\n",
    "                # text_dataを2要素の配列に変換\n",
    "                text_data_array_clap = [text_data, \"\"]\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # model2.get_text_embeddingの前にtext_dataを配列に変換\n",
    "                    text_embed_A = model2.get_text_embedding(text_data_array, use_tensor=True).to(device)\n",
    "                    text_embed_B = model3.get_text_embedding(text_data_array, use_tensor=True).to(device)\n",
    "\n",
    "                    # [2, 512]のTensorから第一要素を取り出し、[1, 512]のTensorにする\n",
    "                    text_embed_A = text_embed_A[0].unsqueeze(0)\n",
    "                    text_embed_B = text_embed_B[0].unsqueeze(0)\n",
    "\n",
    "                    # CUDAデバイス上のテンソルをCPU上のNumPy配列に変換\n",
    "                    text_embed_np_A = text_embed_A.cpu().numpy()\n",
    "                    text_embed_np_B = text_embed_B.cpu().numpy()\n",
    "\n",
    "                # テンソルをNumPy配列に変換してリストに追加\n",
    "                embedding_array_list_A.append(text_embed_np_A)\n",
    "                embedding_array_list_B.append(text_embed_np_B)\n",
    "\n",
    "            except Exception as e:\n",
    "                # エラーが発生した場合、エラー内容を表示して保存する\n",
    "                error_message = str(e)\n",
    "                embedding_array_list_A.append(np.array([float('nan')] * 512))  # エラーの場合はNaNで埋める\n",
    "                embedding_array_list_B.append(np.array([float('nan')] * 512))  # エラーの場合はNaNで埋める\n",
    "                print(f\"エラーが発生しました: {error_message}\")\n",
    "\n",
    "        # NumPy配列のリストを結合して、2次元のNumPy配列に変換\n",
    "        embedding_array_A = np.vstack(embedding_array_list_A)\n",
    "        embedding_array_B = np.vstack(embedding_array_list_B)\n",
    "\n",
    "        # NumPy配列を保存\n",
    "        with open(save_path + \"_A.npy\", 'wb') as f:\n",
    "            np.save(f, embedding_array_A)\n",
    "        with open(save_path + \"_B.npy\", 'wb') as f:\n",
    "            np.save(f, embedding_array_B)\n",
    "\n",
    "        # 結果を表示\n",
    "        print(\"結果が保存されました。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {e}\")\n",
    "\n",
    "# CSVファイル、抽出列、保存パスを指定\n",
    "csv_file_path = \"../Datasets/archive/musiccaps-processed.csv\"\n",
    "target_column_name = \"aspect_list\"\n",
    "numpy_save_path = \"../Datasets/archive/clap_embeds\"\n",
    "\n",
    "clap_process_and_add_embedding(csv_file_path, target_column_name, numpy_save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
