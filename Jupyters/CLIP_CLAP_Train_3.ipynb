{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255a60c9-e477-42d0-818d-8039dc02cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f526074-2f45-4a38-b510-9d8215b301ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a92b2-52bf-497d-b0bf-1ad95da7767d",
   "metadata": {},
   "source": [
    " **Model Train** only train-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ea96b59-24df-4b71-8638-0f39f18c77e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m     78\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m---> 79\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, X_train, y_train, X_val, y_val, epochs, batch_size, model_path)\u001b[0m\n\u001b[1;32m     48\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs, targets)\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=8)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=8)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        return output\n",
    "\n",
    "# データの前処理と分割\n",
    "def preprocess_data(data_file):\n",
    "    data = np.load(data_file)\n",
    "    # nanを含む行を削除\n",
    "    data = data[~np.isnan(data).any(axis=(1,2))]\n",
    "    X = torch.tensor(data[:, 1, :], dtype=torch.float32)\n",
    "    y = torch.tensor(data[:, 0, :], dtype=torch.float32)\n",
    "    y = y.to(X.dtype)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# ハイパーパラメータの最適化（ランダムサーチ）\n",
    "def optimize_hyperparameters(model, X_train, y_train, X_val, y_val):\n",
    "    learning_rate = random.uniform(0.0001, 0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return learning_rate, criterion, optimizer\n",
    "\n",
    "# 学習関数（チェックポイントの保存先パスを指定）\n",
    "def train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, epochs, batch_size, model_path):\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            # targetsのデータタイプをinputsと同じに変換\n",
    "            #targets = targets.to(inputs.dtype)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # バッチごとに損失を計算して保存\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
    "        torch.save(model.state_dict(), os.path.join(model_path, f\"checkpoint_epoch_{epoch+1}.pt\"))\n",
    "\n",
    "# 学習データのファイルパス\n",
    "data_file = \"../Datasets/archive/reshaped_text_embeds.npy\"\n",
    "# モデルの保存先ディレクトリ\n",
    "model_path = \"../Models/CLAP_converter/\"\n",
    "\n",
    "# ディレクトリが存在しない場合は作成\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# モデルの初期化\n",
    "model = TransformerModel()\n",
    "\n",
    "# データの前処理と分割\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocess_data(data_file)\n",
    "\n",
    "# ハイパーパラメータの最適化\n",
    "learning_rate, criterion, optimizer = optimize_hyperparameters(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# モデルの学習\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, epochs, batch_size, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0ecf3fd-7749-4fff-9725-348f14d2d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "torch.float16\n",
      "torch.Size([461, 512])\n",
      "torch.Size([461, 512])\n",
      "torch.Size([461, 512])\n",
      "torch.Size([461, 512])\n"
     ]
    }
   ],
   "source": [
    "data = np.load(data_file)\n",
    "# nanを含む行を削除\n",
    "data = data[~np.isnan(data).any(axis=(1,2))]\n",
    "X = torch.tensor(data[:, 0, :], dtype=torch.float16)\n",
    "y = torch.tensor(data[:, 1, :], dtype=torch.float16)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(X_train.dtype)\n",
    "print(y_train.dtype)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a52602d2-7eeb-41ad-8149-21e897b08930",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# モデルの定義\u001b[39;00m\n\u001b[1;32m    112\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerModel(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# 損失関数と最適化手法の定義\u001b[39;00m\n\u001b[1;32m    116\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layers=6):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(input_dim, nhead=8), num_layers)\n",
    "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(input_dim, nhead=8), num_layers)\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "# データセットの定義\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "# データセットの前処理\n",
    "def preprocess_dataset(dataset):\n",
    "    dataset = dataset[~np.isnan(dataset).any(axis=(1,2))]\n",
    "    x = dataset[:, 0, :-1]  # 入力データ\n",
    "    y = dataset[:, 1, :]    # 教師データ\n",
    "    return x, y\n",
    "\n",
    "# データセットの分割\n",
    "def split_dataset(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "# 学習曲線の保存\n",
    "def save_learning_curve(train_losses, val_losses):\n",
    "    np.save('../Models/ClIP_converter/train_losses.npy', train_losses)\n",
    "    np.save('../Models/ClIP_converter/val_losses.npy', val_losses)\n",
    "\n",
    "# トレーニング関数\n",
    "def train(model, device, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data in train_loader:\n",
    "        inputs = data.to(device)\n",
    "        targets = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "# バリデーション関数\n",
    "def validate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs = data.to(device)\n",
    "            targets = data.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "# メイン関数\n",
    "if __name__ == '__main__':\n",
    "    # 乱数シードの設定\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # デバイスの設定\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # データセットの読み込み\n",
    "    dataset = np.load('../Datasets/archive/reshaped_text_embeds.npy')\n",
    "    \n",
    "    # データセットの前処理\n",
    "    x, y = preprocess_dataset(dataset)\n",
    "    \n",
    "    # データセットの分割\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test = split_dataset(x, y)\n",
    "    \n",
    "    # データローダーの作成\n",
    "    train_dataset = MyDataset(x_train)\n",
    "    val_dataset = MyDataset(x_val)\n",
    "    test_dataset = MyDataset(x_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # モデルの定義\n",
    "    model = TransformerModel(512, 512)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 損失関数と最適化手法の定義\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # モデルの保存先ディレクトリ\n",
    "    model_path = \"../Models/ClIP_converter/\"\n",
    "\n",
    "    # ディレクトリが存在しない場合は作成\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    \n",
    "    # 学習の実施\n",
    "    num_epochs = 10\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, device, train_loader, criterion, optimizer)\n",
    "        val_loss = validate(model, device, val_loader, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # チェックポイントの保存\n",
    "        torch.save(model.state_dict(), os.path.join(model_path, f\"checkpoint_epoch_{epoch+1}.pt\"))\n",
    "    \n",
    "    # 学習曲線の保存\n",
    "    save_learning_curve(train_losses, val_losses)\n",
    "    \n",
    "    # テストデータによる評価\n",
    "    test_loss = validate(model, device, test_loader, criterion)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8581e5c7-2cda-483b-ac45-ab0794f85d97",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(\n",
    "            d_model=input_size, nhead=8), num_layers=num_layers)\n",
    "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(\n",
    "            d_model=input_size, nhead=8), num_layers=num_layers)\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        output = self.linear(decoded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "224016f8-f8a5-4164-baf8-8b7cc4a7bf69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "size() received an invalid combination of arguments - got (int, int), but expected one of:\n * (int dim)\n * ()\n * (name dim)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     29\u001b[0m     torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTensorDataset(val_input, val_target),\n\u001b[1;32m     30\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# ハイパーパラメータの設定\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m output_size \u001b[38;5;241m=\u001b[39m train_target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     35\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: size() received an invalid combination of arguments - got (int, int), but expected one of:\n * (int dim)\n * ()\n * (name dim)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# データセットロード\n",
    "dataset = np.load('../Datasets/archive/reshaped_text_embeds.npy')\n",
    "input_data = torch.Tensor(dataset[:, 0, :])\n",
    "target_data = torch.Tensor(dataset[:, 1, :])\n",
    "\n",
    "# nanを含む要素を取り除く\n",
    "nan_indices = np.unique(np.argwhere(torch.isnan(input_data)).flatten())\n",
    "input_data = torch.cat([input_data[i].unsqueeze(0) for i in range(len(input_data)) if i not in nan_indices], dim=0)\n",
    "target_data = torch.cat([target_data[i].unsqueeze(0) for i in range(len(target_data)) if i not in nan_indices], dim=0)\n",
    "\n",
    "# データセット分割\n",
    "train_input, val_input, train_target, val_target = train_test_split(\n",
    "    input_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(train_input, train_target),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(val_input, val_target),\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "input_size = train_input.size(1,512)\n",
    "output_size = train_target.size(1,512)\n",
    "num_layers = 6\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# モデルの構築\n",
    "model = TransformerModel(input_size, output_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# モデルの保存先ディレクトリ\n",
    "model_path = \"../Models/ClIP_converter/\"\n",
    "\n",
    "# ディレクトリが存在しない場合は作成\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# 学習\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    \n",
    "    # チェックポイントの保存\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, f\"checkpoint_epoch_{epoch+1}.pt\"))\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "# 損失曲線の保存\n",
    "np.save('train_loss.npy', train_loss_list)\n",
    "np.save('val_loss.npy', val_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce185a7-ec1d-492a-b72d-e4fa1d2e07bb",
   "metadata": {},
   "source": [
    "**Model(+PositionEncoding) Train** train-loss + val-loss + best model saving + learnning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7faaa44f-d089-42a1-aa9f-66b87a0df02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.4392726580644476\n",
      "Epoch 1/40, Validation Loss: 0.16008565574884415\n",
      "!!!!Best is now!!!!\n",
      "Epoch 2/40, Loss: 0.12741961021875514\n",
      "Epoch 2/40, Validation Loss: 0.03938715858384967\n",
      "!!!!Best is now!!!!\n",
      "Epoch 3/40, Loss: 0.06867167944538183\n",
      "Epoch 3/40, Validation Loss: 0.02660676441155374\n",
      "!!!!Best is now!!!!\n",
      "Epoch 4/40, Loss: 0.06283803823693045\n",
      "Epoch 4/40, Validation Loss: 0.027331993682309985\n",
      "Epoch 5/40, Loss: 0.06119704246520996\n",
      "Epoch 5/40, Validation Loss: 0.027256166096776724\n",
      "Epoch 6/40, Loss: 0.056904605750379894\n",
      "Epoch 6/40, Validation Loss: 0.027539798989892006\n",
      "Epoch 7/40, Loss: 0.053731458197379935\n",
      "Epoch 7/40, Validation Loss: 0.027708352776244283\n",
      "Epoch 8/40, Loss: 0.05092694289211569\n",
      "Epoch 8/40, Validation Loss: 0.02739057596772909\n",
      "Epoch 9/40, Loss: 0.048957952523025976\n",
      "Epoch 9/40, Validation Loss: 0.02700996329076588\n",
      "Epoch 10/40, Loss: 0.0462046486550364\n",
      "Epoch 10/40, Validation Loss: 0.02690472360700369\n",
      "Epoch 11/40, Loss: 0.045081893303270996\n",
      "Epoch 11/40, Validation Loss: 0.026986719109117985\n",
      "Epoch 12/40, Loss: 0.04317792774788264\n",
      "Epoch 12/40, Validation Loss: 0.026785368099808693\n",
      "Epoch 13/40, Loss: 0.041687295868478975\n",
      "Epoch 13/40, Validation Loss: 0.027182978577911854\n",
      "Epoch 14/40, Loss: 0.040561828376918005\n",
      "Epoch 14/40, Validation Loss: 0.02635869337245822\n",
      "!!!!Best is now!!!!\n",
      "Epoch 15/40, Loss: 0.03950269957040918\n",
      "Epoch 15/40, Validation Loss: 0.026606869185343385\n",
      "Epoch 16/40, Loss: 0.038314304089751734\n",
      "Epoch 16/40, Validation Loss: 0.026911748107522726\n",
      "Epoch 17/40, Loss: 0.03768864017108391\n",
      "Epoch 17/40, Validation Loss: 0.026814489159733057\n",
      "Epoch 18/40, Loss: 0.036401064873769366\n",
      "Epoch 18/40, Validation Loss: 0.027083919616416097\n",
      "Epoch 19/40, Loss: 0.03571551049063946\n",
      "Epoch 19/40, Validation Loss: 0.026237569749355316\n",
      "!!!!Best is now!!!!\n",
      "Epoch 20/40, Loss: 0.03505066551011184\n",
      "Epoch 20/40, Validation Loss: 0.026445220690220594\n",
      "Epoch 21/40, Loss: 0.0346611849963665\n",
      "Epoch 21/40, Validation Loss: 0.02639750111848116\n",
      "Epoch 22/40, Loss: 0.03395951237401058\n",
      "Epoch 22/40, Validation Loss: 0.026889359578490257\n",
      "Epoch 23/40, Loss: 0.03355455507749114\n",
      "Epoch 23/40, Validation Loss: 0.02673035254701972\n",
      "Epoch 24/40, Loss: 0.03322124995034317\n",
      "Epoch 24/40, Validation Loss: 0.026528963120654225\n",
      "Epoch 25/40, Loss: 0.03249935388308147\n",
      "Epoch 25/40, Validation Loss: 0.026349682128056884\n",
      "Epoch 26/40, Loss: 0.03200129111265314\n",
      "Epoch 26/40, Validation Loss: 0.026142441667616367\n",
      "!!!!Best is now!!!!\n",
      "Epoch 27/40, Loss: 0.03178097089302951\n",
      "Epoch 27/40, Validation Loss: 0.026562837418168783\n",
      "Epoch 28/40, Loss: 0.031051878271431757\n",
      "Epoch 28/40, Validation Loss: 0.026348564075306058\n",
      "Epoch 29/40, Loss: 0.030953966081142426\n",
      "Epoch 29/40, Validation Loss: 0.02619489305652678\n",
      "Epoch 30/40, Loss: 0.030742208505498952\n",
      "Epoch 30/40, Validation Loss: 0.02664765063673258\n",
      "Epoch 31/40, Loss: 0.03009579529793098\n",
      "Epoch 31/40, Validation Loss: 0.026377473957836628\n",
      "Epoch 32/40, Loss: 0.030012663325359082\n",
      "Epoch 32/40, Validation Loss: 0.026372453663498163\n",
      "Epoch 33/40, Loss: 0.029636655002832413\n",
      "Epoch 33/40, Validation Loss: 0.026280487421900034\n",
      "Epoch 34/40, Loss: 0.029458605780683714\n",
      "Epoch 34/40, Validation Loss: 0.02662437316030264\n",
      "Epoch 35/40, Loss: 0.02919319650993265\n",
      "Epoch 35/40, Validation Loss: 0.026547978399321437\n",
      "Epoch 36/40, Loss: 0.028899207839678073\n",
      "Epoch 36/40, Validation Loss: 0.026796998223289847\n",
      "Epoch 37/40, Loss: 0.028663812462112\n",
      "Epoch 37/40, Validation Loss: 0.026149673853069544\n",
      "Epoch 38/40, Loss: 0.02855118325558202\n",
      "Epoch 38/40, Validation Loss: 0.026182386558502913\n",
      "Epoch 39/40, Loss: 0.028482037628519124\n",
      "Epoch 39/40, Validation Loss: 0.026372318854555488\n",
      "Epoch 40/40, Loss: 0.028249125542311834\n",
      "Epoch 40/40, Validation Loss: 0.026677575195208192\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Transformerモデルの定義\n",
    "import math\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, device, max_sequence_length, d_model=512, nhead=8, num_layers=8):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.positional_encoding = self.generate_positional_encoding(max_sequence_length, d_model)\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        # 位置エンコーディングを加算\n",
    "        src = src + self.positional_encoding[:src.size(0), :].to(self.device)\n",
    "        tgt = tgt + self.positional_encoding[:tgt.size(0), :].to(self.device)\n",
    "        \n",
    "        memory = self.encoder(src.to(self.device))\n",
    "        output = self.decoder(tgt.to(self.device), memory)\n",
    "        return output\n",
    "    \n",
    "    def generate_positional_encoding(self, max_sequence_length, d_model):\n",
    "        position = torch.arange(0, max_sequence_length).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        positional_encoding = torch.zeros(max_sequence_length, d_model)\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return positional_encoding\n",
    "\n",
    "\n",
    "# データの前処理と分割\n",
    "def preprocess_data(data_file):\n",
    "    data = np.load(data_file)\n",
    "    # nanを含む行を削除\n",
    "    data = data[~np.isnan(data).any(axis=(1, 2))]\n",
    "    X = torch.tensor(data[:, 1, :], dtype=torch.float32)\n",
    "    y = torch.tensor(data[:, 0, :], dtype=torch.float32)\n",
    "    y = y.to(X.dtype)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    #X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train,X_test, y_test\n",
    "\n",
    "# ハイパーパラメータの最適化（ランダムサーチ）\n",
    "def optimize_hyperparameters(model):\n",
    "    learning_rate = random.uniform(0.0001, 0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return learning_rate, criterion, optimizer\n",
    "\n",
    "# 精度の計算\n",
    "def calculate_accuracy(model, criterion, X_test, y_test):\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs, targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# 学習関数（チェックポイントと精度の保存先パスを指定）\n",
    "def train_model(model, criterion, optimizer, X_train, y_train, X_test, y_test, epochs, batch_size, model_path):\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    avg_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train().to(device)\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, targets)\n",
    "            outputs = model(inputs.to(device), targets.to(device))\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # バッチごとに損失を計算\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "        # テストデータで損失を計算\n",
    "        val_loss = calculate_accuracy(model, criterion, X_test, y_test)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss}\")\n",
    "\n",
    "        avg_losses.append(avg_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # 精度が改善した場合はモデルを保存\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(model_path, \"best_model.pt\"))\n",
    "            print(\"!!!!Best is now!!!!\")\n",
    "\n",
    "    # 学習曲線を保存\n",
    "    np.save(os.path.join(model_path, \"learning_curve.npy\"), np.array([avg_losses, val_losses]))\n",
    "\n",
    "    # 学習終了後、モデルを保存\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, \"final_model.pt\"))\n",
    "\n",
    "    # 学習曲線を出力\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(range(1, epochs+1), avg_losses, label='Train Loss')\n",
    "    plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # 学習曲線を出力\n",
    "    import matplotlib.pyplot as plt\n",
    "    import japanize_matplotlib\n",
    "    #from matplotlib.font_manager import FontProperties\n",
    "    #matplotlib.font_manager.FontProperties(fname = フォント,size = フォントサイズ )\n",
    "    plt.plot(range(1, epochs+1), avg_losses, label='訓練データでの損失')\n",
    "    plt.plot(range(1, epochs+1), val_losses, label='試験データでの損失')\n",
    "    plt.xlabel('学習回数')\n",
    "    plt.ylabel('損失（平均二乗誤差）')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 学習データのファイルパス\n",
    "data_file = \"../Datasets/archive/reshaped_text_embeds.npy\"\n",
    "# モデルの保存先ディレクトリ\n",
    "model_path = \"../Models/CLAP_converter/\"\n",
    "\n",
    "# ディレクトリが存在しない場合は作成\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# CUDAが利用可能かチェック\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# モデルの初期化\n",
    "model = TransformerModel(device, 512)#.to(device)\n",
    "if device == \"cuda\":\n",
    "    model = model.cuda()\n",
    "\n",
    "# データの前処理と分割\n",
    "X_train, y_train, X_test, y_test = preprocess_data(data_file)\n",
    "\n",
    "# ハイパーパラメータの最適化\n",
    "learning_rate, criterion, optimizer = optimize_hyperparameters(model)\n",
    "\n",
    "# モデルの学習\n",
    "epochs = 40\n",
    "batch_size = 128\n",
    "train_model(model, criterion, optimizer, X_train, y_train, X_test, y_test, epochs, batch_size, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e86fb6-bb3b-42b0-8ff3-0569e8a22507",
   "metadata": {},
   "source": [
    "**Model Load Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81045825-0d8a-40f7-99ce-effe8dcd09bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([1, 512])\n",
      "Sample of output tensor: tensor([[-0.0407,  0.0420, -0.0433,  0.0037, -0.0062, -0.0019,  0.0267,  0.0348,\n",
      "          0.0090, -0.0205,  0.0404, -0.0121, -0.0046, -0.0036,  0.0285,  0.0110,\n",
      "         -0.0206,  0.0065,  0.0068, -0.0349, -0.0176,  0.0030,  0.0074,  0.0229,\n",
      "         -0.0210,  0.0029, -0.0183, -0.0021,  0.0298, -0.0150, -0.0274,  0.0070,\n",
      "          0.0160, -0.0021,  0.0084,  0.0241, -0.0079, -0.0246,  0.0521,  0.0162,\n",
      "          0.0813,  0.0542,  0.0023, -0.0403,  0.0340, -0.0165, -0.0344, -0.0055,\n",
      "          0.0346, -0.0241, -0.0031,  0.0134,  0.0007,  0.0206, -0.0029,  0.0435,\n",
      "         -0.0192,  0.0019,  0.0210, -0.0099, -0.0242,  0.0012,  0.0096,  0.0271,\n",
      "         -0.0080, -0.0056,  0.0030,  0.0211, -0.0122,  0.0196, -0.0419, -0.0130,\n",
      "          0.0002, -0.0432, -0.0140, -0.0406,  0.0286, -0.0083, -0.0769, -0.0689,\n",
      "          0.0386, -0.0707, -0.0088, -0.0280, -0.0173,  0.0089,  0.0195,  0.0104,\n",
      "          0.0401,  0.0102, -0.0321, -0.0255, -0.0439, -0.0119,  0.0029,  0.0208,\n",
      "         -0.0566, -0.0169, -0.0047,  0.0311, -0.0135,  0.0240,  0.0201,  0.0351,\n",
      "          0.0248, -0.0340,  0.0106, -0.0089, -0.0129, -0.0078, -0.0075, -0.0827,\n",
      "         -0.0238,  0.0021,  0.0362, -0.0138,  0.0286, -0.0069,  0.0081,  0.0171,\n",
      "          0.0105, -0.0358, -0.0417, -0.0335,  0.0445,  0.0413,  0.0196, -0.0162,\n",
      "         -0.0091, -0.0234,  0.0221, -0.0124, -0.0243,  0.0369,  0.0395, -0.0234,\n",
      "          0.0460,  0.0213,  0.0684, -0.0154,  0.0551, -0.0384, -0.0328,  0.0267,\n",
      "         -0.0060, -0.0036,  0.0133, -0.0266,  0.0127,  0.0153, -0.0386, -0.0273,\n",
      "          0.0348,  0.0466, -0.0178, -0.0323, -0.0157, -0.0490, -0.0066,  0.0200,\n",
      "          0.0130, -0.0502,  0.0095, -0.0023,  0.0100, -0.0337,  0.0422, -0.0237,\n",
      "          0.0082,  0.0288,  0.0503,  0.0491,  0.0407,  0.0031, -0.0393, -0.0223,\n",
      "          0.0408, -0.0140, -0.0072, -0.0196, -0.0226, -0.0754, -0.0165,  0.0195,\n",
      "          0.0186,  0.0391, -0.0220,  0.0163,  0.0221,  0.0019,  0.0134, -0.0193,\n",
      "          0.0431,  0.0364,  0.0213,  0.0077,  0.0123,  0.0340,  0.0170,  0.0290,\n",
      "          0.0095,  0.0285,  0.0004,  0.0046,  0.0326,  0.0117,  0.0289,  0.0539,\n",
      "         -0.0206, -0.0256, -0.0520,  0.0278,  0.0191, -0.0223,  0.0280,  0.0039,\n",
      "         -0.0126, -0.0413, -0.0534, -0.0251,  0.0125,  0.0103, -0.0511, -0.0240,\n",
      "         -0.0687,  0.0017,  0.0326,  0.0112,  0.0289,  0.0693, -0.0064,  0.0047,\n",
      "          0.0192, -0.0431, -0.0245,  0.0436, -0.0022, -0.0150, -0.0397,  0.0510,\n",
      "          0.0306, -0.0178,  0.0070,  0.0093, -0.0085, -0.0288,  0.0140, -0.0003,\n",
      "          0.0590,  0.0289, -0.0285, -0.0202,  0.0350,  0.0181,  0.0293,  0.0200,\n",
      "         -0.0660,  0.0280,  0.0342, -0.0042, -0.0356, -0.0334,  0.0097, -0.0631,\n",
      "          0.0512, -0.0415, -0.0105,  0.0239,  0.0041, -0.0398,  0.0170,  0.0154,\n",
      "         -0.0150, -0.0393, -0.0128, -0.0184, -0.0666,  0.0356,  0.0127, -0.0307,\n",
      "         -0.0170,  0.0107, -0.0101, -0.0238,  0.0653,  0.0227, -0.0039, -0.0114,\n",
      "         -0.0095, -0.0510,  0.0156,  0.0638, -0.0073, -0.0546,  0.0228, -0.0009,\n",
      "         -0.0039,  0.0211, -0.0349,  0.0274,  0.0478, -0.0215,  0.0074,  0.0338,\n",
      "         -0.0067, -0.0423,  0.0239,  0.0112, -0.0032, -0.0575,  0.0203, -0.0255,\n",
      "          0.0159, -0.0147, -0.0186,  0.0005, -0.0189, -0.0190, -0.0078,  0.0009,\n",
      "         -0.0152, -0.0411, -0.0152,  0.0278,  0.0130, -0.0515, -0.0449, -0.0043,\n",
      "          0.0257,  0.0191,  0.0184, -0.0417,  0.0121, -0.0494, -0.0062, -0.0479,\n",
      "          0.0177,  0.0101, -0.0117,  0.0453, -0.0508, -0.0385, -0.0228, -0.0016,\n",
      "          0.0432, -0.0163,  0.0048, -0.0256,  0.0378, -0.0041,  0.0011,  0.0380,\n",
      "         -0.0179,  0.0339,  0.0295,  0.0238,  0.0560, -0.0252, -0.0535,  0.0531,\n",
      "         -0.0069, -0.0232, -0.0370,  0.0297,  0.0263, -0.0242,  0.0259, -0.0241,\n",
      "          0.0019,  0.0089,  0.0471,  0.0058,  0.0260, -0.0064,  0.0004,  0.0132,\n",
      "         -0.0017, -0.0295, -0.0078, -0.0362, -0.0309,  0.0160,  0.0106,  0.0185,\n",
      "         -0.0452,  0.0033, -0.0171,  0.0339, -0.0070,  0.0026,  0.0216,  0.0379,\n",
      "          0.0088, -0.0405,  0.0272,  0.0008,  0.0511,  0.0283, -0.0096, -0.0244,\n",
      "         -0.0023,  0.0086,  0.0576,  0.0329, -0.0148,  0.0338, -0.0099,  0.0157,\n",
      "         -0.0431,  0.0016,  0.0137, -0.0729,  0.0192,  0.0097, -0.0104, -0.0160,\n",
      "          0.0128,  0.0389,  0.0027,  0.0122, -0.0388, -0.0128, -0.0115, -0.0422,\n",
      "         -0.0125, -0.0001,  0.0181, -0.0147,  0.0082,  0.0212, -0.0357, -0.0155,\n",
      "          0.0253, -0.0487, -0.0029,  0.0148,  0.0362,  0.0521, -0.0403, -0.0489,\n",
      "         -0.0369, -0.0063,  0.0453, -0.0178,  0.0335, -0.0225, -0.0165, -0.0386,\n",
      "          0.0514, -0.0246,  0.0262,  0.0134,  0.0034, -0.0507,  0.0196, -0.0208,\n",
      "         -0.0364,  0.0031, -0.0158,  0.0020, -0.0047, -0.0584, -0.0140, -0.0266,\n",
      "          0.0086,  0.0265, -0.0149,  0.0207,  0.0111,  0.0447, -0.0138, -0.0141,\n",
      "          0.0139,  0.0062,  0.0158, -0.0348, -0.0338,  0.0118, -0.0306, -0.0280,\n",
      "         -0.0497, -0.0051,  0.0167, -0.0055, -0.0283,  0.0245, -0.0109, -0.0030,\n",
      "         -0.0445, -0.0122, -0.0166,  0.0053, -0.0176,  0.0195, -0.0589, -0.0294,\n",
      "          0.0056, -0.0108, -0.0095, -0.0054, -0.0120, -0.0462, -0.0362, -0.0430,\n",
      "          0.0149,  0.0045,  0.0048,  0.0309,  0.0517, -0.0219, -0.0083,  0.0338]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "\n",
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=6)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        return output\n",
    "\n",
    "# 学習後に保存されたモデルのパス\n",
    "model_path = \"../Models/CLAP_converter/best_model.pt\"\n",
    "\n",
    "# モデルの初期化\n",
    "model = TransformerModel()\n",
    "# GPUを利用可能であればGPUにモデルを転送\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# テスト用のランダムな(1,512)のテンソルを生成\n",
    "random_input = torch.randn(1, 512).to(device)\n",
    "\n",
    "# モデルにランダムな入力を与えてテスト\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(random_input, random_input)\n",
    "\n",
    "# 出力を表示\n",
    "print(\"Output tensor shape:\", output.shape)\n",
    "print(\"Sample of output tensor:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a5e2e-80f4-41ec-ba02-342f342658a2",
   "metadata": {},
   "source": [
    "**CLIP+Converter vs CLAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa98753e-7e19-4a15-b45d-028a99e5d9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 77.9kB/s]\n",
      "vocab.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 553kB/s]\n",
      "tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 935kB/s]\n",
      "config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 1.89MB/s]\n",
      "vocab.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 1.85MB/s]\n",
      "merges.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 2.20MB/s]\n",
      "tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 18.3MB/s]\n",
      "config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 481/481 [00:00<00:00, 1.30MB/s]\n",
      "vocab.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 24.9MB/s]\n",
      "merges.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 4.37MB/s]\n",
      "tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 51.5MB/s]\n",
      "config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1.72k/1.72k [00:00<00:00, 4.57MB/s]\n",
      "100%|███████████████████████████████████████| 338M/338M [00:19<00:00, 18.3MiB/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201336/work/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "model.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████| 499M/499M [00:09<00:00, 51.5MB/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the specified checkpoint ../Models/Laion_CLAP/music_audioset_epoch_15_esc_90.14.patched.pt from users.\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import laion_clap\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "model1, preprocess1 = clip.load(\"ViT-B/32\", device=device)\n",
    "model2 = laion_clap.CLAP_Module(enable_fusion=False, amodel= 'HTSAT-base')\n",
    "model2.load_ckpt('../Models/Laion_CLAP/music_audioset_epoch_15_esc_90.14.patched.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54d6640a-109f-4c41-a4d7-4f42b83d6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.float16\n",
      "torch.float32\n",
      "torch.Size([1, 512])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "text_data = \"a cat\"\n",
    "with torch.no_grad():\n",
    "    # model2.get_text_embeddingの前にtext_dataを配列に変換\n",
    "    token = clip.tokenize(text_data).to(device)\n",
    "    text_embed_clip = model1.encode_text(token)\n",
    "    print(text_embed_clip.shape)\n",
    "    print(text_embed_clip.dtype)\n",
    "    text_embed_clip = text_embed_clip.to(torch.float32)\n",
    "    print(text_embed_clip.dtype)\n",
    "\n",
    "    # text_dataを2要素の配列に変換\n",
    "    text_data_array = [text_data, \"\"]\n",
    "    text_embed_clap = model2.get_text_embedding(text_data_array, use_tensor=True).to(device)\n",
    "    # [2, 512]のTensorから第一要素を取り出し、[1, 512]のTensorにする\n",
    "    text_embed_clap = text_embed_clap[0].unsqueeze(0)\n",
    "    print(text_embed_clap.shape)\n",
    "    print(text_embed_clap.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "196cd3ca-900f-475a-8e81-cba770be4938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([1, 512])\n",
      "Output tensor dtype: torch.float32\n",
      "tensor(0.0020, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# モデルにランダムな入力を与えてテスト\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(text_embed_clip, text_embed_clip)\n",
    "\n",
    "# 出力を表示\n",
    "print(\"Output tensor shape:\", output.shape)\n",
    "print(\"Output tensor dtype:\", output.dtype)\n",
    "#print(\"Sample of output tensor:\", output)\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, text_embed_clap)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "892599d6-b383-4d43-b6a7-0bfa0722dd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02b760-4f40-44aa-b0e7-972d8354706e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "226b2676-f8ab-4a60-ad41-0cd03f10fe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd94f6-68a8-4157-865b-0abd4c5fb579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
