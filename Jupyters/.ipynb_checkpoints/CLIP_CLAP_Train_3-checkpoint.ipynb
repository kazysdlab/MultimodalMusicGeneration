{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255a60c9-e477-42d0-818d-8039dc02cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f526074-2f45-4a38-b510-9d8215b301ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a92b2-52bf-497d-b0bf-1ad95da7767d",
   "metadata": {},
   "source": [
    " **Model Train** only train-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ea96b59-24df-4b71-8638-0f39f18c77e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 79\u001b[0m\n\u001b[1;32m     77\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m     78\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m---> 79\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, X_train, y_train, X_val, y_val, epochs, batch_size, model_path)\u001b[0m\n\u001b[1;32m     48\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs, targets)\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=8)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=8)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        return output\n",
    "\n",
    "# データの前処理と分割\n",
    "def preprocess_data(data_file):\n",
    "    data = np.load(data_file)\n",
    "    # nanを含む行を削除\n",
    "    data = data[~np.isnan(data).any(axis=(1,2))]\n",
    "    X = torch.tensor(data[:, 1, :], dtype=torch.float32)\n",
    "    y = torch.tensor(data[:, 0, :], dtype=torch.float32)\n",
    "    y = y.to(X.dtype)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# ハイパーパラメータの最適化（ランダムサーチ）\n",
    "def optimize_hyperparameters(model, X_train, y_train, X_val, y_val):\n",
    "    learning_rate = random.uniform(0.0001, 0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return learning_rate, criterion, optimizer\n",
    "\n",
    "# 学習関数（チェックポイントの保存先パスを指定）\n",
    "def train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, epochs, batch_size, model_path):\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            # targetsのデータタイプをinputsと同じに変換\n",
    "            #targets = targets.to(inputs.dtype)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # バッチごとに損失を計算して保存\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
    "        torch.save(model.state_dict(), os.path.join(model_path, f\"checkpoint_epoch_{epoch+1}.pt\"))\n",
    "\n",
    "# 学習データのファイルパス\n",
    "data_file = \"../Datasets/archive/reshaped_text_embeds.npy\"\n",
    "# モデルの保存先ディレクトリ\n",
    "model_path = \"../Models/CLAP_converter/\"\n",
    "\n",
    "# ディレクトリが存在しない場合は作成\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# モデルの初期化\n",
    "model = TransformerModel()\n",
    "\n",
    "# データの前処理と分割\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocess_data(data_file)\n",
    "\n",
    "# ハイパーパラメータの最適化\n",
    "learning_rate, criterion, optimizer = optimize_hyperparameters(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# モデルの学習\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, epochs, batch_size, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0ecf3fd-7749-4fff-9725-348f14d2d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "torch.float16\n",
      "torch.Size([461, 512])\n",
      "torch.Size([461, 512])\n",
      "torch.Size([461, 512])\n",
      "torch.Size([461, 512])\n"
     ]
    }
   ],
   "source": [
    "data = np.load(data_file)\n",
    "# nanを含む行を削除\n",
    "data = data[~np.isnan(data).any(axis=(1,2))]\n",
    "X = torch.tensor(data[:, 0, :], dtype=torch.float16)\n",
    "y = torch.tensor(data[:, 1, :], dtype=torch.float16)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(X_train.dtype)\n",
    "print(y_train.dtype)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a52602d2-7eeb-41ad-8149-21e897b08930",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# モデルの定義\u001b[39;00m\n\u001b[1;32m    112\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerModel(\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# 損失関数と最適化手法の定義\u001b[39;00m\n\u001b[1;32m    116\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 810 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layers=6):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(input_dim, nhead=8), num_layers)\n",
    "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(input_dim, nhead=8), num_layers)\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "# データセットの定義\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "# データセットの前処理\n",
    "def preprocess_dataset(dataset):\n",
    "    dataset = dataset[~np.isnan(dataset).any(axis=(1,2))]\n",
    "    x = dataset[:, 0, :-1]  # 入力データ\n",
    "    y = dataset[:, 1, :]    # 教師データ\n",
    "    return x, y\n",
    "\n",
    "# データセットの分割\n",
    "def split_dataset(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "# 学習曲線の保存\n",
    "def save_learning_curve(train_losses, val_losses):\n",
    "    np.save('../Models/ClIP_converter/train_losses.npy', train_losses)\n",
    "    np.save('../Models/ClIP_converter/val_losses.npy', val_losses)\n",
    "\n",
    "# トレーニング関数\n",
    "def train(model, device, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data in train_loader:\n",
    "        inputs = data.to(device)\n",
    "        targets = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "# バリデーション関数\n",
    "def validate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs = data.to(device)\n",
    "            targets = data.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "# メイン関数\n",
    "if __name__ == '__main__':\n",
    "    # 乱数シードの設定\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # デバイスの設定\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # データセットの読み込み\n",
    "    dataset = np.load('../Datasets/archive/reshaped_text_embeds.npy')\n",
    "    \n",
    "    # データセットの前処理\n",
    "    x, y = preprocess_dataset(dataset)\n",
    "    \n",
    "    # データセットの分割\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test = split_dataset(x, y)\n",
    "    \n",
    "    # データローダーの作成\n",
    "    train_dataset = MyDataset(x_train)\n",
    "    val_dataset = MyDataset(x_val)\n",
    "    test_dataset = MyDataset(x_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # モデルの定義\n",
    "    model = TransformerModel(512, 512)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 損失関数と最適化手法の定義\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # モデルの保存先ディレクトリ\n",
    "    model_path = \"../Models/ClIP_converter/\"\n",
    "\n",
    "    # ディレクトリが存在しない場合は作成\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    \n",
    "    # 学習の実施\n",
    "    num_epochs = 10\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, device, train_loader, criterion, optimizer)\n",
    "        val_loss = validate(model, device, val_loader, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # チェックポイントの保存\n",
    "        torch.save(model.state_dict(), os.path.join(model_path, f\"checkpoint_epoch_{epoch+1}.pt\"))\n",
    "    \n",
    "    # 学習曲線の保存\n",
    "    save_learning_curve(train_losses, val_losses)\n",
    "    \n",
    "    # テストデータによる評価\n",
    "    test_loss = validate(model, device, test_loader, criterion)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8581e5c7-2cda-483b-ac45-ab0794f85d97",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(\n",
    "            d_model=input_size, nhead=8), num_layers=num_layers)\n",
    "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(\n",
    "            d_model=input_size, nhead=8), num_layers=num_layers)\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        output = self.linear(decoded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "224016f8-f8a5-4164-baf8-8b7cc4a7bf69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "size() received an invalid combination of arguments - got (int, int), but expected one of:\n * (int dim)\n * ()\n * (name dim)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     29\u001b[0m     torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTensorDataset(val_input, val_target),\n\u001b[1;32m     30\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# ハイパーパラメータの設定\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m output_size \u001b[38;5;241m=\u001b[39m train_target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     35\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: size() received an invalid combination of arguments - got (int, int), but expected one of:\n * (int dim)\n * ()\n * (name dim)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# データセットロード\n",
    "dataset = np.load('../Datasets/archive/reshaped_text_embeds.npy')\n",
    "input_data = torch.Tensor(dataset[:, 0, :])\n",
    "target_data = torch.Tensor(dataset[:, 1, :])\n",
    "\n",
    "# nanを含む要素を取り除く\n",
    "nan_indices = np.unique(np.argwhere(torch.isnan(input_data)).flatten())\n",
    "input_data = torch.cat([input_data[i].unsqueeze(0) for i in range(len(input_data)) if i not in nan_indices], dim=0)\n",
    "target_data = torch.cat([target_data[i].unsqueeze(0) for i in range(len(target_data)) if i not in nan_indices], dim=0)\n",
    "\n",
    "# データセット分割\n",
    "train_input, val_input, train_target, val_target = train_test_split(\n",
    "    input_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(train_input, train_target),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(val_input, val_target),\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "input_size = train_input.size(1,512)\n",
    "output_size = train_target.size(1,512)\n",
    "num_layers = 6\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# モデルの構築\n",
    "model = TransformerModel(input_size, output_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# モデルの保存先ディレクトリ\n",
    "model_path = \"../Models/ClIP_converter/\"\n",
    "\n",
    "# ディレクトリが存在しない場合は作成\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# 学習\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    \n",
    "    # チェックポイントの保存\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, f\"checkpoint_epoch_{epoch+1}.pt\"))\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "# 損失曲線の保存\n",
    "np.save('train_loss.npy', train_loss_list)\n",
    "np.save('val_loss.npy', val_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce185a7-ec1d-492a-b72d-e4fa1d2e07bb",
   "metadata": {},
   "source": [
    "**Model Train** train-loss + val-loss + best model saving + learnning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7faaa44f-d089-42a1-aa9f-66b87a0df02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch 1/40, Loss: 0.42321504703883467\n",
      "Epoch 1/40, Validation Loss: 0.3420342244207859\n",
      "!!!!Best is now!!!!\n",
      "Epoch 2/40, Loss: 0.33925645515836517\n",
      "Epoch 2/40, Validation Loss: 0.32193151861429214\n",
      "!!!!Best is now!!!!\n",
      "Epoch 3/40, Loss: 0.3164810558845257\n",
      "Epoch 3/40, Validation Loss: 0.30255309492349625\n",
      "!!!!Best is now!!!!\n",
      "Epoch 4/40, Loss: 0.2959305582375362\n",
      "Epoch 4/40, Validation Loss: 0.28420892357826233\n",
      "!!!!Best is now!!!!\n",
      "Epoch 5/40, Loss: 0.27720272438279514\n",
      "Epoch 5/40, Validation Loss: 0.2658296823501587\n",
      "!!!!Best is now!!!!\n",
      "Epoch 6/40, Loss: 0.25898229099553205\n",
      "Epoch 6/40, Validation Loss: 0.24859133176505566\n",
      "!!!!Best is now!!!!\n",
      "Epoch 7/40, Loss: 0.24204004478865657\n",
      "Epoch 7/40, Validation Loss: 0.23254736326634884\n",
      "!!!!Best is now!!!!\n",
      "Epoch 8/40, Loss: 0.2256523211454523\n",
      "Epoch 8/40, Validation Loss: 0.21659585647284985\n",
      "!!!!Best is now!!!!\n",
      "Epoch 9/40, Loss: 0.2103608183819672\n",
      "Epoch 9/40, Validation Loss: 0.2016744501888752\n",
      "!!!!Best is now!!!!\n",
      "Epoch 10/40, Loss: 0.19571781620897097\n",
      "Epoch 10/40, Validation Loss: 0.18720580451190472\n",
      "!!!!Best is now!!!!\n",
      "Epoch 11/40, Loss: 0.18178486104669242\n",
      "Epoch 11/40, Validation Loss: 0.17348909936845303\n",
      "!!!!Best is now!!!!\n",
      "Epoch 12/40, Loss: 0.16857465228130078\n",
      "Epoch 12/40, Validation Loss: 0.16063388995826244\n",
      "!!!!Best is now!!!!\n",
      "Epoch 13/40, Loss: 0.155791404946097\n",
      "Epoch 13/40, Validation Loss: 0.14845646545290947\n",
      "!!!!Best is now!!!!\n",
      "Epoch 14/40, Loss: 0.14374578256031562\n",
      "Epoch 14/40, Validation Loss: 0.13703486695885658\n",
      "!!!!Best is now!!!!\n",
      "Epoch 15/40, Loss: 0.13248969254822568\n",
      "Epoch 15/40, Validation Loss: 0.12542700488120317\n",
      "!!!!Best is now!!!!\n",
      "Epoch 16/40, Loss: 0.12156536738420355\n",
      "Epoch 16/40, Validation Loss: 0.11496647819876671\n",
      "!!!!Best is now!!!!\n",
      "Epoch 17/40, Loss: 0.11147925565982687\n",
      "Epoch 17/40, Validation Loss: 0.1053639343008399\n",
      "!!!!Best is now!!!!\n",
      "Epoch 18/40, Loss: 0.10183835671893482\n",
      "Epoch 18/40, Validation Loss: 0.09574711881577969\n",
      "!!!!Best is now!!!!\n",
      "Epoch 19/40, Loss: 0.09272246550897072\n",
      "Epoch 19/40, Validation Loss: 0.0867750495672226\n",
      "!!!!Best is now!!!!\n",
      "Epoch 20/40, Loss: 0.08422933878569767\n",
      "Epoch 20/40, Validation Loss: 0.07880403846502304\n",
      "!!!!Best is now!!!!\n",
      "Epoch 21/40, Loss: 0.07626871610509939\n",
      "Epoch 21/40, Validation Loss: 0.07074662763625383\n",
      "!!!!Best is now!!!!\n",
      "Epoch 22/40, Loss: 0.06874705478549004\n",
      "Epoch 22/40, Validation Loss: 0.06364264665171504\n",
      "!!!!Best is now!!!!\n",
      "Epoch 23/40, Loss: 0.0619423959789605\n",
      "Epoch 23/40, Validation Loss: 0.05705601768568158\n",
      "!!!!Best is now!!!!\n",
      "Epoch 24/40, Loss: 0.05567052446562668\n",
      "Epoch 24/40, Validation Loss: 0.05130941281095147\n",
      "!!!!Best is now!!!!\n",
      "Epoch 25/40, Loss: 0.04998110632958083\n",
      "Epoch 25/40, Validation Loss: 0.04552649846300483\n",
      "!!!!Best is now!!!!\n",
      "Epoch 26/40, Loss: 0.044132759730363715\n",
      "Epoch 26/40, Validation Loss: 0.03975020069628954\n",
      "!!!!Best is now!!!!\n",
      "Epoch 27/40, Loss: 0.03857726038529955\n",
      "Epoch 27/40, Validation Loss: 0.034759856294840574\n",
      "!!!!Best is now!!!!\n",
      "Epoch 28/40, Loss: 0.03438361117552067\n",
      "Epoch 28/40, Validation Loss: 0.031191677786409855\n",
      "!!!!Best is now!!!!\n",
      "Epoch 29/40, Loss: 0.031520502384880494\n",
      "Epoch 29/40, Validation Loss: 0.029372189193964005\n",
      "!!!!Best is now!!!!\n",
      "Epoch 30/40, Loss: 0.02992404759700956\n",
      "Epoch 30/40, Validation Loss: 0.02790210535749793\n",
      "!!!!Best is now!!!!\n",
      "Epoch 31/40, Loss: 0.030299864580919003\n",
      "Epoch 31/40, Validation Loss: 0.027513421373441815\n",
      "!!!!Best is now!!!!\n",
      "Epoch 32/40, Loss: 0.028954520693113064\n",
      "Epoch 32/40, Validation Loss: 0.02692826301790774\n",
      "!!!!Best is now!!!!\n",
      "Epoch 33/40, Loss: 0.028540216514776492\n",
      "Epoch 33/40, Validation Loss: 0.02711921138688922\n",
      "Epoch 34/40, Loss: 0.028310409586491257\n",
      "Epoch 34/40, Validation Loss: 0.02698746300302446\n",
      "Epoch 35/40, Loss: 0.028143378231545974\n",
      "Epoch 35/40, Validation Loss: 0.026949810795485973\n",
      "Epoch 36/40, Loss: 0.028038991550947058\n",
      "Epoch 36/40, Validation Loss: 0.026918365387246013\n",
      "!!!!Best is now!!!!\n",
      "Epoch 37/40, Loss: 0.027931653669682043\n",
      "Epoch 37/40, Validation Loss: 0.026862134924158454\n",
      "!!!!Best is now!!!!\n",
      "Epoch 38/40, Loss: 0.027886008908008707\n",
      "Epoch 38/40, Validation Loss: 0.02680279314517975\n",
      "!!!!Best is now!!!!\n",
      "Epoch 39/40, Loss: 0.027844464599058545\n",
      "Epoch 39/40, Validation Loss: 0.02689081896096468\n",
      "Epoch 40/40, Loss: 0.027837627021403147\n",
      "Epoch 40/40, Validation Loss: 0.026706508360803127\n",
      "!!!!Best is now!!!!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=8)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=8)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        memory = self.encoder(src.to(self.device))\n",
    "        output = self.decoder(tgt.to(self.device), memory)\n",
    "        return output\n",
    "\n",
    "\n",
    "# データの前処理と分割\n",
    "def preprocess_data(data_file):\n",
    "    data = np.load(data_file)\n",
    "    # nanを含む行を削除\n",
    "    data = data[~np.isnan(data).any(axis=(1, 2))]\n",
    "    X = torch.tensor(data[:, 1, :], dtype=torch.float32)\n",
    "    y = torch.tensor(data[:, 0, :], dtype=torch.float32)\n",
    "    y = y.to(X.dtype)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    #X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train,X_test, y_test\n",
    "\n",
    "# ハイパーパラメータの最適化（ランダムサーチ）\n",
    "def optimize_hyperparameters(model):\n",
    "    learning_rate = random.uniform(0.0001, 0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return learning_rate, criterion, optimizer\n",
    "\n",
    "# 精度の計算\n",
    "def calculate_accuracy(model, criterion, X_test, y_test):\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs, targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# 学習関数（チェックポイントと精度の保存先パスを指定）\n",
    "def train_model(model, criterion, optimizer, X_train, y_train, X_test, y_test, epochs, batch_size, model_path):\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    avg_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train().to(device)\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, targets)\n",
    "            outputs = model(inputs.to(device), targets.to(device))\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # バッチごとに損失を計算\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "        # テストデータで損失を計算\n",
    "        val_loss = calculate_accuracy(model, criterion, X_test, y_test)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss}\")\n",
    "\n",
    "        avg_losses.append(avg_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # 精度が改善した場合はモデルを保存\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(model_path, \"best_model.pt\"))\n",
    "            print(\"!!!!Best is now!!!!\")\n",
    "\n",
    "    # 学習曲線を保存\n",
    "    np.save(os.path.join(model_path, \"learning_curve.npy\"), np.array([avg_losses, val_losses]))\n",
    "\n",
    "    # 学習終了後、モデルを保存\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, \"final_model.pt\"))\n",
    "\n",
    "    # 学習曲線を出力\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(range(1, epochs+1), avg_losses, label='Train Loss')\n",
    "    plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 学習データのファイルパス\n",
    "data_file = \"../Datasets/archive/reshaped_text_embeds.npy\"\n",
    "# モデルの保存先ディレクトリ\n",
    "model_path = \"../Models/CLAP_converter/\"\n",
    "\n",
    "# ディレクトリが存在しない場合は作成\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# CUDAが利用可能かチェック\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# モデルの初期化\n",
    "model = TransformerModel(device)#.to(device)\n",
    "if device == \"cuda\":\n",
    "    model = model.cuda()\n",
    "\n",
    "# データの前処理と分割\n",
    "X_train, y_train, X_test, y_test = preprocess_data(data_file)\n",
    "\n",
    "# ハイパーパラメータの最適化\n",
    "learning_rate, criterion, optimizer = optimize_hyperparameters(model)\n",
    "\n",
    "# モデルの学習\n",
    "epochs = 40\n",
    "batch_size = 128\n",
    "train_model(model, criterion, optimizer, X_train, y_train, X_test, y_test, epochs, batch_size, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e86fb6-bb3b-42b0-8ff3-0569e8a22507",
   "metadata": {},
   "source": [
    "**Model Load Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81045825-0d8a-40f7-99ce-effe8dcd09bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([1, 512])\n",
      "Sample of output tensor: tensor([[-0.0407,  0.0420, -0.0433,  0.0037, -0.0062, -0.0019,  0.0267,  0.0348,\n",
      "          0.0090, -0.0205,  0.0404, -0.0121, -0.0046, -0.0036,  0.0285,  0.0110,\n",
      "         -0.0206,  0.0065,  0.0068, -0.0349, -0.0176,  0.0030,  0.0074,  0.0229,\n",
      "         -0.0210,  0.0029, -0.0183, -0.0021,  0.0298, -0.0150, -0.0274,  0.0070,\n",
      "          0.0160, -0.0021,  0.0084,  0.0241, -0.0079, -0.0246,  0.0521,  0.0162,\n",
      "          0.0813,  0.0542,  0.0023, -0.0403,  0.0340, -0.0165, -0.0344, -0.0055,\n",
      "          0.0346, -0.0241, -0.0031,  0.0134,  0.0007,  0.0206, -0.0029,  0.0435,\n",
      "         -0.0192,  0.0019,  0.0210, -0.0099, -0.0242,  0.0012,  0.0096,  0.0271,\n",
      "         -0.0080, -0.0056,  0.0030,  0.0211, -0.0122,  0.0196, -0.0419, -0.0130,\n",
      "          0.0002, -0.0432, -0.0140, -0.0406,  0.0286, -0.0083, -0.0769, -0.0689,\n",
      "          0.0386, -0.0707, -0.0088, -0.0280, -0.0173,  0.0089,  0.0195,  0.0104,\n",
      "          0.0401,  0.0102, -0.0321, -0.0255, -0.0439, -0.0119,  0.0029,  0.0208,\n",
      "         -0.0566, -0.0169, -0.0047,  0.0311, -0.0135,  0.0240,  0.0201,  0.0351,\n",
      "          0.0248, -0.0340,  0.0106, -0.0089, -0.0129, -0.0078, -0.0075, -0.0827,\n",
      "         -0.0238,  0.0021,  0.0362, -0.0138,  0.0286, -0.0069,  0.0081,  0.0171,\n",
      "          0.0105, -0.0358, -0.0417, -0.0335,  0.0445,  0.0413,  0.0196, -0.0162,\n",
      "         -0.0091, -0.0234,  0.0221, -0.0124, -0.0243,  0.0369,  0.0395, -0.0234,\n",
      "          0.0460,  0.0213,  0.0684, -0.0154,  0.0551, -0.0384, -0.0328,  0.0267,\n",
      "         -0.0060, -0.0036,  0.0133, -0.0266,  0.0127,  0.0153, -0.0386, -0.0273,\n",
      "          0.0348,  0.0466, -0.0178, -0.0323, -0.0157, -0.0490, -0.0066,  0.0200,\n",
      "          0.0130, -0.0502,  0.0095, -0.0023,  0.0100, -0.0337,  0.0422, -0.0237,\n",
      "          0.0082,  0.0288,  0.0503,  0.0491,  0.0407,  0.0031, -0.0393, -0.0223,\n",
      "          0.0408, -0.0140, -0.0072, -0.0196, -0.0226, -0.0754, -0.0165,  0.0195,\n",
      "          0.0186,  0.0391, -0.0220,  0.0163,  0.0221,  0.0019,  0.0134, -0.0193,\n",
      "          0.0431,  0.0364,  0.0213,  0.0077,  0.0123,  0.0340,  0.0170,  0.0290,\n",
      "          0.0095,  0.0285,  0.0004,  0.0046,  0.0326,  0.0117,  0.0289,  0.0539,\n",
      "         -0.0206, -0.0256, -0.0520,  0.0278,  0.0191, -0.0223,  0.0280,  0.0039,\n",
      "         -0.0126, -0.0413, -0.0534, -0.0251,  0.0125,  0.0103, -0.0511, -0.0240,\n",
      "         -0.0687,  0.0017,  0.0326,  0.0112,  0.0289,  0.0693, -0.0064,  0.0047,\n",
      "          0.0192, -0.0431, -0.0245,  0.0436, -0.0022, -0.0150, -0.0397,  0.0510,\n",
      "          0.0306, -0.0178,  0.0070,  0.0093, -0.0085, -0.0288,  0.0140, -0.0003,\n",
      "          0.0590,  0.0289, -0.0285, -0.0202,  0.0350,  0.0181,  0.0293,  0.0200,\n",
      "         -0.0660,  0.0280,  0.0342, -0.0042, -0.0356, -0.0334,  0.0097, -0.0631,\n",
      "          0.0512, -0.0415, -0.0105,  0.0239,  0.0041, -0.0398,  0.0170,  0.0154,\n",
      "         -0.0150, -0.0393, -0.0128, -0.0184, -0.0666,  0.0356,  0.0127, -0.0307,\n",
      "         -0.0170,  0.0107, -0.0101, -0.0238,  0.0653,  0.0227, -0.0039, -0.0114,\n",
      "         -0.0095, -0.0510,  0.0156,  0.0638, -0.0073, -0.0546,  0.0228, -0.0009,\n",
      "         -0.0039,  0.0211, -0.0349,  0.0274,  0.0478, -0.0215,  0.0074,  0.0338,\n",
      "         -0.0067, -0.0423,  0.0239,  0.0112, -0.0032, -0.0575,  0.0203, -0.0255,\n",
      "          0.0159, -0.0147, -0.0186,  0.0005, -0.0189, -0.0190, -0.0078,  0.0009,\n",
      "         -0.0152, -0.0411, -0.0152,  0.0278,  0.0130, -0.0515, -0.0449, -0.0043,\n",
      "          0.0257,  0.0191,  0.0184, -0.0417,  0.0121, -0.0494, -0.0062, -0.0479,\n",
      "          0.0177,  0.0101, -0.0117,  0.0453, -0.0508, -0.0385, -0.0228, -0.0016,\n",
      "          0.0432, -0.0163,  0.0048, -0.0256,  0.0378, -0.0041,  0.0011,  0.0380,\n",
      "         -0.0179,  0.0339,  0.0295,  0.0238,  0.0560, -0.0252, -0.0535,  0.0531,\n",
      "         -0.0069, -0.0232, -0.0370,  0.0297,  0.0263, -0.0242,  0.0259, -0.0241,\n",
      "          0.0019,  0.0089,  0.0471,  0.0058,  0.0260, -0.0064,  0.0004,  0.0132,\n",
      "         -0.0017, -0.0295, -0.0078, -0.0362, -0.0309,  0.0160,  0.0106,  0.0185,\n",
      "         -0.0452,  0.0033, -0.0171,  0.0339, -0.0070,  0.0026,  0.0216,  0.0379,\n",
      "          0.0088, -0.0405,  0.0272,  0.0008,  0.0511,  0.0283, -0.0096, -0.0244,\n",
      "         -0.0023,  0.0086,  0.0576,  0.0329, -0.0148,  0.0338, -0.0099,  0.0157,\n",
      "         -0.0431,  0.0016,  0.0137, -0.0729,  0.0192,  0.0097, -0.0104, -0.0160,\n",
      "          0.0128,  0.0389,  0.0027,  0.0122, -0.0388, -0.0128, -0.0115, -0.0422,\n",
      "         -0.0125, -0.0001,  0.0181, -0.0147,  0.0082,  0.0212, -0.0357, -0.0155,\n",
      "          0.0253, -0.0487, -0.0029,  0.0148,  0.0362,  0.0521, -0.0403, -0.0489,\n",
      "         -0.0369, -0.0063,  0.0453, -0.0178,  0.0335, -0.0225, -0.0165, -0.0386,\n",
      "          0.0514, -0.0246,  0.0262,  0.0134,  0.0034, -0.0507,  0.0196, -0.0208,\n",
      "         -0.0364,  0.0031, -0.0158,  0.0020, -0.0047, -0.0584, -0.0140, -0.0266,\n",
      "          0.0086,  0.0265, -0.0149,  0.0207,  0.0111,  0.0447, -0.0138, -0.0141,\n",
      "          0.0139,  0.0062,  0.0158, -0.0348, -0.0338,  0.0118, -0.0306, -0.0280,\n",
      "         -0.0497, -0.0051,  0.0167, -0.0055, -0.0283,  0.0245, -0.0109, -0.0030,\n",
      "         -0.0445, -0.0122, -0.0166,  0.0053, -0.0176,  0.0195, -0.0589, -0.0294,\n",
      "          0.0056, -0.0108, -0.0095, -0.0054, -0.0120, -0.0462, -0.0362, -0.0430,\n",
      "          0.0149,  0.0045,  0.0048,  0.0309,  0.0517, -0.0219, -0.0083,  0.0338]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "\n",
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=6)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        return output\n",
    "\n",
    "# 学習後に保存されたモデルのパス\n",
    "model_path = \"../Models/CLAP_converter/best_model.pt\"\n",
    "\n",
    "# モデルの初期化\n",
    "model = TransformerModel()\n",
    "# GPUを利用可能であればGPUにモデルを転送\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# テスト用のランダムな(1,512)のテンソルを生成\n",
    "random_input = torch.randn(1, 512).to(device)\n",
    "\n",
    "# モデルにランダムな入力を与えてテスト\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(random_input, random_input)\n",
    "\n",
    "# 出力を表示\n",
    "print(\"Output tensor shape:\", output.shape)\n",
    "print(\"Sample of output tensor:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a5e2e-80f4-41ec-ba02-342f342658a2",
   "metadata": {},
   "source": [
    "**CLIP+Converter vs CLAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa98753e-7e19-4a15-b45d-028a99e5d9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 77.9kB/s]\n",
      "vocab.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 553kB/s]\n",
      "tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 935kB/s]\n",
      "config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 1.89MB/s]\n",
      "vocab.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 1.85MB/s]\n",
      "merges.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 2.20MB/s]\n",
      "tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 18.3MB/s]\n",
      "config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 481/481 [00:00<00:00, 1.30MB/s]\n",
      "vocab.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 24.9MB/s]\n",
      "merges.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 4.37MB/s]\n",
      "tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 51.5MB/s]\n",
      "config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1.72k/1.72k [00:00<00:00, 4.57MB/s]\n",
      "100%|███████████████████████████████████████| 338M/338M [00:19<00:00, 18.3MiB/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201336/work/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "model.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████| 499M/499M [00:09<00:00, 51.5MB/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the specified checkpoint ../Models/Laion_CLAP/music_audioset_epoch_15_esc_90.14.patched.pt from users.\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import laion_clap\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "model1, preprocess1 = clip.load(\"ViT-B/32\", device=device)\n",
    "model2 = laion_clap.CLAP_Module(enable_fusion=False, amodel= 'HTSAT-base')\n",
    "model2.load_ckpt('../Models/Laion_CLAP/music_audioset_epoch_15_esc_90.14.patched.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54d6640a-109f-4c41-a4d7-4f42b83d6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.float16\n",
      "torch.float32\n",
      "torch.Size([1, 512])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "text_data = \"a cat\"\n",
    "with torch.no_grad():\n",
    "    # model2.get_text_embeddingの前にtext_dataを配列に変換\n",
    "    token = clip.tokenize(text_data).to(device)\n",
    "    text_embed_clip = model1.encode_text(token)\n",
    "    print(text_embed_clip.shape)\n",
    "    print(text_embed_clip.dtype)\n",
    "    text_embed_clip = text_embed_clip.to(torch.float32)\n",
    "    print(text_embed_clip.dtype)\n",
    "\n",
    "    # text_dataを2要素の配列に変換\n",
    "    text_data_array = [text_data, \"\"]\n",
    "    text_embed_clap = model2.get_text_embedding(text_data_array, use_tensor=True).to(device)\n",
    "    # [2, 512]のTensorから第一要素を取り出し、[1, 512]のTensorにする\n",
    "    text_embed_clap = text_embed_clap[0].unsqueeze(0)\n",
    "    print(text_embed_clap.shape)\n",
    "    print(text_embed_clap.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "196cd3ca-900f-475a-8e81-cba770be4938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([1, 512])\n",
      "Output tensor dtype: torch.float32\n",
      "tensor(0.0020, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# モデルにランダムな入力を与えてテスト\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(text_embed_clip, text_embed_clip)\n",
    "\n",
    "# 出力を表示\n",
    "print(\"Output tensor shape:\", output.shape)\n",
    "print(\"Output tensor dtype:\", output.dtype)\n",
    "#print(\"Sample of output tensor:\", output)\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, text_embed_clap)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "892599d6-b383-4d43-b6a7-0bfa0722dd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02b760-4f40-44aa-b0e7-972d8354706e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "226b2676-f8ab-4a60-ad41-0cd03f10fe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.1'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd94f6-68a8-4157-865b-0abd4c5fb579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
