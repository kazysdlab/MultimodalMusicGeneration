{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "255a60c9-e477-42d0-818d-8039dc02cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f526074-2f45-4a38-b510-9d8215b301ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a92b2-52bf-497d-b0bf-1ad95da7767d",
   "metadata": {},
   "source": [
    " **Model Train** only train-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ea96b59-24df-4b71-8638-0f39f18c77e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.7905016644247647\n",
      "Epoch 2/100, Loss: 0.4769673203599864\n",
      "Epoch 3/100, Loss: 0.23303348879361974\n",
      "Epoch 4/100, Loss: 0.08313925050455949\n",
      "Epoch 5/100, Loss: 0.026643522276446736\n",
      "Epoch 6/100, Loss: 0.008585720804744753\n",
      "Epoch 7/100, Loss: 0.003387983976702752\n",
      "Epoch 8/100, Loss: 0.0018617285130513382\n",
      "Epoch 9/100, Loss: 0.001520178983277031\n",
      "Epoch 10/100, Loss: 0.0013893735494689438\n",
      "Epoch 11/100, Loss: 0.0013532646658734001\n",
      "Epoch 12/100, Loss: 0.0013291308824936378\n",
      "Epoch 13/100, Loss: 0.0013110842214544014\n",
      "Epoch 14/100, Loss: 0.0012840134179990354\n",
      "Epoch 15/100, Loss: 0.0012678501079790294\n",
      "Epoch 16/100, Loss: 0.0012862601734954736\n",
      "Epoch 17/100, Loss: 0.004547456287425654\n",
      "Epoch 18/100, Loss: 0.0024091863486883714\n",
      "Epoch 19/100, Loss: 0.0017170013342020583\n",
      "Epoch 20/100, Loss: 0.0016277255490422249\n",
      "Epoch 21/100, Loss: 0.0016474628556070142\n",
      "Epoch 22/100, Loss: 0.0015154895593476449\n",
      "Epoch 23/100, Loss: 0.0015407965060901538\n",
      "Epoch 24/100, Loss: 0.0014461243810164261\n",
      "Epoch 25/100, Loss: 0.0014165149246952657\n",
      "Epoch 26/100, Loss: 0.0014413304863786646\n",
      "Epoch 27/100, Loss: 0.001382093679898515\n",
      "Epoch 28/100, Loss: 0.0013391509479521934\n",
      "Epoch 29/100, Loss: 0.0013246049498336326\n",
      "Epoch 30/100, Loss: 0.0013113028935863283\n",
      "Epoch 31/100, Loss: 0.0012919327428821347\n",
      "Epoch 32/100, Loss: 0.0012844438455870439\n",
      "Epoch 33/100, Loss: 0.0013413437630114113\n",
      "Epoch 34/100, Loss: 0.001276172978546599\n",
      "Epoch 35/100, Loss: 0.001268702804046715\n",
      "Epoch 36/100, Loss: 0.0012845935135405383\n",
      "Epoch 37/100, Loss: 0.0012647644576520242\n",
      "Epoch 38/100, Loss: 0.0012633371074555505\n",
      "Epoch 39/100, Loss: 0.0012562010128147386\n",
      "Epoch 40/100, Loss: 0.0012596012133269988\n",
      "Epoch 41/100, Loss: 0.0012524978153343345\n",
      "Epoch 42/100, Loss: 0.001399982056243281\n",
      "Epoch 43/100, Loss: 0.0014192307616808805\n",
      "Epoch 44/100, Loss: 0.0013952834013817382\n",
      "Epoch 45/100, Loss: 0.0012686592024943695\n",
      "Epoch 46/100, Loss: 0.00126697546963034\n",
      "Epoch 47/100, Loss: 0.001251688758955048\n",
      "Epoch 48/100, Loss: 0.001266811159439385\n",
      "Epoch 49/100, Loss: 0.002263643951476391\n",
      "Epoch 50/100, Loss: 0.0015658423593588944\n",
      "Epoch 51/100, Loss: 0.0014645296313543\n",
      "Epoch 52/100, Loss: 0.001566708465267358\n",
      "Epoch 53/100, Loss: 0.0014227851261866503\n",
      "Epoch 54/100, Loss: 0.001421342212465945\n",
      "Epoch 55/100, Loss: 0.0013966150059974913\n",
      "Epoch 56/100, Loss: 0.001446058843204559\n",
      "Epoch 57/100, Loss: 0.0014127138600652587\n",
      "Epoch 58/100, Loss: 0.001433002001782558\n",
      "Epoch 59/100, Loss: 0.0013794854708850898\n",
      "Epoch 60/100, Loss: 0.0014526932460159577\n",
      "Epoch 61/100, Loss: 0.0014291842114822618\n",
      "Epoch 62/100, Loss: 0.0013388619750160083\n",
      "Epoch 63/100, Loss: 0.0013710581336234663\n",
      "Epoch 64/100, Loss: 0.0013398968931769245\n",
      "Epoch 65/100, Loss: 0.0013766638224077378\n",
      "Epoch 66/100, Loss: 0.0013409812637636888\n",
      "Epoch 67/100, Loss: 0.0013435987819885385\n",
      "Epoch 68/100, Loss: 0.0013333054737123694\n",
      "Epoch 69/100, Loss: 0.0013366609004904226\n",
      "Epoch 70/100, Loss: 0.001334470293307998\n",
      "Epoch 71/100, Loss: 0.001320955178957304\n",
      "Epoch 72/100, Loss: 0.0013116665830803585\n",
      "Epoch 73/100, Loss: 0.001292228192925967\n",
      "Epoch 74/100, Loss: 0.0013162498502863635\n",
      "Epoch 75/100, Loss: 0.0013046480177355737\n",
      "Epoch 76/100, Loss: 0.001298956247849454\n",
      "Epoch 77/100, Loss: 0.0012874390331803467\n",
      "Epoch 78/100, Loss: 0.0012949871116093006\n",
      "Epoch 79/100, Loss: 0.001282903552456791\n",
      "Epoch 80/100, Loss: 0.0012772457256656268\n",
      "Epoch 81/100, Loss: 0.0012835751604770534\n",
      "Epoch 82/100, Loss: 0.0012952003202497445\n",
      "Epoch 83/100, Loss: 0.0012734300368626055\n",
      "Epoch 84/100, Loss: 0.0012749537287665338\n",
      "Epoch 85/100, Loss: 0.0012734606882139783\n",
      "Epoch 86/100, Loss: 0.0012692963054146745\n",
      "Epoch 87/100, Loss: 0.0012701754759322724\n",
      "Epoch 88/100, Loss: 0.001268317959078684\n",
      "Epoch 89/100, Loss: 0.0012620003404224228\n",
      "Epoch 90/100, Loss: 0.0012608782151424937\n",
      "Epoch 91/100, Loss: 0.0012578464590880121\n",
      "Epoch 92/100, Loss: 0.0012546134630924668\n",
      "Epoch 93/100, Loss: 0.001260872504770242\n",
      "Epoch 94/100, Loss: 0.0012571352718654893\n",
      "Epoch 95/100, Loss: 0.0012544203902883776\n",
      "Epoch 96/100, Loss: 0.001256628281131772\n",
      "Epoch 97/100, Loss: 0.0012569934759963431\n",
      "Epoch 98/100, Loss: 0.0012533916220679108\n",
      "Epoch 99/100, Loss: 0.0012828242395008947\n",
      "Epoch 100/100, Loss: 0.0012882184739031926\n"
     ]
    }
   ],
   "source": [
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=6)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        return output\n",
    "\n",
    "# データの前処理と分割\n",
    "def preprocess_data(data_file):\n",
    "    data = np.load(data_file)\n",
    "    # nanを含む行を削除\n",
    "    data = data[~np.isnan(data).any(axis=(1,2))]\n",
    "    X = torch.tensor(data[:, 0, :], dtype=torch.float32)\n",
    "    y = torch.tensor(data[:, 1, :], dtype=torch.float32)\n",
    "    y = y.to(X.dtype)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# ハイパーパラメータの最適化（ランダムサーチ）\n",
    "def optimize_hyperparameters(model, X_train, y_train, X_val, y_val):\n",
    "    learning_rate = random.uniform(0.0001, 0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return learning_rate, criterion, optimizer\n",
    "\n",
    "# 学習関数（チェックポイントの保存先パスを指定）\n",
    "def train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, epochs, batch_size, model_path):\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            # targetsのデータタイプをinputsと同じに変換\n",
    "            #targets = targets.to(inputs.dtype)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # バッチごとに損失を計算して保存\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
    "        torch.save(model.state_dict(), os.path.join(model_path, f\"checkpoint_epoch_{epoch+1}.pt\"))\n",
    "\n",
    "# 学習データのファイルパス\n",
    "data_file = \"../Datasets/archive/reshaped_text_embeds.npy\"\n",
    "# モデルの保存先ディレクトリ\n",
    "model_path = \"../Models/ClIP_converter/\"\n",
    "\n",
    "# ディレクトリが存在しない場合は作成\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# モデルの初期化\n",
    "model = TransformerModel()\n",
    "\n",
    "# データの前処理と分割\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocess_data(data_file)\n",
    "\n",
    "# ハイパーパラメータの最適化\n",
    "learning_rate, criterion, optimizer = optimize_hyperparameters(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# モデルの学習\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, epochs, batch_size, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0ecf3fd-7749-4fff-9725-348f14d2d9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "torch.float16\n",
      "torch.Size([461, 512])\n",
      "torch.Size([461, 512])\n",
      "torch.Size([461, 512])\n",
      "torch.Size([461, 512])\n"
     ]
    }
   ],
   "source": [
    "data = np.load(data_file)\n",
    "# nanを含む行を削除\n",
    "data = data[~np.isnan(data).any(axis=(1,2))]\n",
    "X = torch.tensor(data[:, 0, :], dtype=torch.float16)\n",
    "y = torch.tensor(data[:, 1, :], dtype=torch.float16)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(X_train.dtype)\n",
    "print(y_train.dtype)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a52602d2-7eeb-41ad-8149-21e897b08930",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "was expecting embedding dimension of 512, but got 511",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 130\u001b[0m\n\u001b[1;32m    128\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m--> 130\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate(model, device, val_loader, criterion)\n\u001b[1;32m    132\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[15], line 60\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     57\u001b[0m targets \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 60\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     62\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x)\n\u001b[1;32m     19\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:387\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    384\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 387\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    390\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:707\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    705\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    708\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    714\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5280\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5275\u001b[0m         \u001b[38;5;66;03m# We have the attn_mask, and use that to merge kpm into it.\u001b[39;00m\n\u001b[1;32m   5276\u001b[0m         \u001b[38;5;66;03m# Turn off use of is_causal hint, as the merged mask is no\u001b[39;00m\n\u001b[1;32m   5277\u001b[0m         \u001b[38;5;66;03m# longer causal.\u001b[39;00m\n\u001b[1;32m   5278\u001b[0m         is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 5280\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m embed_dim \u001b[38;5;241m==\u001b[39m embed_dim_to_check, \\\n\u001b[1;32m   5281\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas expecting embedding dimension of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_dim, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m   5283\u001b[0m     \u001b[38;5;66;03m# embed_dim can be a tensor when JIT tracing\u001b[39;00m\n\u001b[1;32m   5284\u001b[0m     head_dim \u001b[38;5;241m=\u001b[39m embed_dim\u001b[38;5;241m.\u001b[39mdiv(num_heads, rounding_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrunc\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: was expecting embedding dimension of 512, but got 511"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_layers=6):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(input_dim, nhead=8), num_layers)\n",
    "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(input_dim, nhead=8), num_layers)\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "# データセットの定義\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "# データセットの前処理\n",
    "def preprocess_dataset(dataset):\n",
    "    dataset = dataset[~np.isnan(dataset).any(axis=(1,2))]\n",
    "    x = dataset[:, 0, :-1]  # 入力データ\n",
    "    y = dataset[:, 1, :]    # 教師データ\n",
    "    return x, y\n",
    "\n",
    "# データセットの分割\n",
    "def split_dataset(x, y):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "# 学習曲線の保存\n",
    "def save_learning_curve(train_losses, val_losses):\n",
    "    np.save('../Models/ClIP_converter/train_losses.npy', train_losses)\n",
    "    np.save('../Models/ClIP_converter/val_losses.npy', val_losses)\n",
    "\n",
    "# トレーニング関数\n",
    "def train(model, device, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for data in train_loader:\n",
    "        inputs = data.to(device)\n",
    "        targets = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(train_loader)\n",
    "\n",
    "# バリデーション関数\n",
    "def validate(model, device, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            inputs = data.to(device)\n",
    "            targets = data.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "# メイン関数\n",
    "if __name__ == '__main__':\n",
    "    # 乱数シードの設定\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # デバイスの設定\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # データセットの読み込み\n",
    "    dataset = np.load('../Datasets/archive/reshaped_text_embeds.npy')\n",
    "    \n",
    "    # データセットの前処理\n",
    "    x, y = preprocess_dataset(dataset)\n",
    "    \n",
    "    # データセットの分割\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test = split_dataset(x, y)\n",
    "    \n",
    "    # データローダーの作成\n",
    "    train_dataset = MyDataset(x_train)\n",
    "    val_dataset = MyDataset(x_val)\n",
    "    test_dataset = MyDataset(x_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # モデルの定義\n",
    "    model = TransformerModel(512, 512)\n",
    "    model.to(device)\n",
    "    \n",
    "    # 損失関数と最適化手法の定義\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # モデルの保存先ディレクトリ\n",
    "    model_path = \"../Models/ClIP_converter/\"\n",
    "\n",
    "    # ディレクトリが存在しない場合は作成\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    \n",
    "    # 学習の実施\n",
    "    num_epochs = 10\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, device, train_loader, criterion, optimizer)\n",
    "        val_loss = validate(model, device, val_loader, criterion)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # チェックポイントの保存\n",
    "        torch.save(model.state_dict(), os.path.join(model_path, f\"checkpoint_epoch_{epoch+1}.pt\"))\n",
    "    \n",
    "    # 学習曲線の保存\n",
    "    save_learning_curve(train_losses, val_losses)\n",
    "    \n",
    "    # テストデータによる評価\n",
    "    test_loss = validate(model, device, test_loader, criterion)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8581e5c7-2cda-483b-ac45-ab0794f85d97",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(\n",
    "            d_model=input_size, nhead=8), num_layers=num_layers)\n",
    "        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(\n",
    "            d_model=input_size, nhead=8), num_layers=num_layers)\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        output = self.linear(decoded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "224016f8-f8a5-4164-baf8-8b7cc4a7bf69",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "size() received an invalid combination of arguments - got (int, int), but expected one of:\n * (int dim)\n * ()\n * (name dim)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 33\u001b[0m\n\u001b[1;32m     28\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     29\u001b[0m     torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTensorDataset(val_input, val_target),\n\u001b[1;32m     30\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# ハイパーパラメータの設定\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m input_size \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m output_size \u001b[38;5;241m=\u001b[39m train_target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m     35\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: size() received an invalid combination of arguments - got (int, int), but expected one of:\n * (int dim)\n * ()\n * (name dim)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# データセットロード\n",
    "dataset = np.load('../Datasets/archive/reshaped_text_embeds.npy')\n",
    "input_data = torch.Tensor(dataset[:, 0, :])\n",
    "target_data = torch.Tensor(dataset[:, 1, :])\n",
    "\n",
    "# nanを含む要素を取り除く\n",
    "nan_indices = np.unique(np.argwhere(torch.isnan(input_data)).flatten())\n",
    "input_data = torch.cat([input_data[i].unsqueeze(0) for i in range(len(input_data)) if i not in nan_indices], dim=0)\n",
    "target_data = torch.cat([target_data[i].unsqueeze(0) for i in range(len(target_data)) if i not in nan_indices], dim=0)\n",
    "\n",
    "# データセット分割\n",
    "train_input, val_input, train_target, val_target = train_test_split(\n",
    "    input_data, target_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 16\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(train_input, train_target),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(val_input, val_target),\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "input_size = train_input.size(1,512)\n",
    "output_size = train_target.size(1,512)\n",
    "num_layers = 6\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# モデルの構築\n",
    "model = TransformerModel(input_size, output_size, num_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# モデルの保存先ディレクトリ\n",
    "model_path = \"../Models/ClIP_converter/\"\n",
    "\n",
    "# ディレクトリが存在しない場合は作成\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# 学習\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    train_loss_list.append(train_loss)\n",
    "    val_loss_list.append(val_loss)\n",
    "    \n",
    "    # チェックポイントの保存\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, f\"checkpoint_epoch_{epoch+1}.pt\"))\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "# 損失曲線の保存\n",
    "np.save('train_loss.npy', train_loss_list)\n",
    "np.save('val_loss.npy', val_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce185a7-ec1d-492a-b72d-e4fa1d2e07bb",
   "metadata": {},
   "source": [
    "**Model Train** train-loss + val-loss + best model saving + learnning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7faaa44f-d089-42a1-aa9f-66b87a0df02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Epoch 1/20, Loss: 0.5964320733629423\n",
      "Epoch 1/20, Validation Loss: 0.2682914175093174\n",
      "Epoch 2/20, Loss: 0.12197788859364288\n",
      "Epoch 2/20, Validation Loss: 0.011819765088148415\n",
      "Epoch 3/20, Loss: 0.010125590391970914\n",
      "Epoch 3/20, Validation Loss: 0.0014335463056340814\n",
      "Epoch 4/20, Loss: 0.005484669830585862\n",
      "Epoch 4/20, Validation Loss: 0.0013040810299571604\n",
      "Epoch 5/20, Loss: 0.0030858503084565544\n",
      "Epoch 5/20, Validation Loss: 0.0013026828091824427\n",
      "Epoch 6/20, Loss: 0.00213244228206318\n",
      "Epoch 6/20, Validation Loss: 0.0013259629922686145\n",
      "Epoch 7/20, Loss: 0.001845698623018789\n",
      "Epoch 7/20, Validation Loss: 0.001265464918105863\n",
      "Epoch 8/20, Loss: 0.0016534715100063075\n",
      "Epoch 8/20, Validation Loss: 0.0012798247917089611\n",
      "Epoch 9/20, Loss: 0.0015818183591332414\n",
      "Epoch 9/20, Validation Loss: 0.0012697417550953105\n",
      "Epoch 10/20, Loss: 0.0015176001189533494\n",
      "Epoch 10/20, Validation Loss: 0.0012661668588407338\n",
      "Epoch 11/20, Loss: 0.0014697222946340154\n",
      "Epoch 11/20, Validation Loss: 0.0012423726730048656\n",
      "Epoch 12/20, Loss: 0.0014306367254526965\n",
      "Epoch 12/20, Validation Loss: 0.0012638882035389543\n",
      "Epoch 13/20, Loss: 0.0014111391802575312\n",
      "Epoch 13/20, Validation Loss: 0.001225725092808716\n",
      "Epoch 14/20, Loss: 0.0013781000101328667\n",
      "Epoch 14/20, Validation Loss: 0.0012322510301601142\n",
      "Epoch 15/20, Loss: 0.001363911720169387\n",
      "Epoch 15/20, Validation Loss: 0.001250812885700725\n",
      "Epoch 16/20, Loss: 0.0013471214835339322\n",
      "Epoch 16/20, Validation Loss: 0.0012334275961620733\n",
      "Epoch 17/20, Loss: 0.0013257667024876795\n",
      "Epoch 17/20, Validation Loss: 0.0012189301778562367\n",
      "Epoch 18/20, Loss: 0.0013155915055841464\n",
      "Epoch 18/20, Validation Loss: 0.001231778718647547\n",
      "Epoch 19/20, Loss: 0.0013036162307453823\n",
      "Epoch 19/20, Validation Loss: 0.0012286604614928365\n",
      "Epoch 20/20, Loss: 0.0012965161345321044\n",
      "Epoch 20/20, Validation Loss: 0.0012277943460503593\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=6)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        return output\n",
    "\n",
    "# データの前処理と分割\n",
    "def preprocess_data(data_file):\n",
    "    data = np.load(data_file)\n",
    "    # nanを含む行を削除\n",
    "    data = data[~np.isnan(data).any(axis=(1, 2))]\n",
    "    X = torch.tensor(data[:, 0, :], dtype=torch.float32)\n",
    "    y = torch.tensor(data[:, 1, :], dtype=torch.float32)\n",
    "    y = y.to(X.dtype)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# ハイパーパラメータの最適化（ランダムサーチ）\n",
    "def optimize_hyperparameters(model, X_train, y_train, X_val, y_val):\n",
    "    learning_rate = random.uniform(0.0001, 0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    return learning_rate, criterion, optimizer\n",
    "\n",
    "# 精度の計算\n",
    "def calculate_accuracy(model, criterion, X_test, y_test):\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs, targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    return avg_loss\n",
    "\n",
    "# 学習関数（チェックポイントと精度の保存先パスを指定）\n",
    "def train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, X_test, y_test, epochs, batch_size, model_path):\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    avg_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, targets)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # バッチごとに損失を計算\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "        # テストデータで損失を計算\n",
    "        val_loss = calculate_accuracy(model, criterion, X_test, y_test)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss}\")\n",
    "\n",
    "        avg_losses.append(avg_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # 精度が改善した場合はモデルを保存\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(model_path, \"best_model.pt\"))\n",
    "\n",
    "    # 学習曲線を保存\n",
    "    np.save(os.path.join(model_path, \"learning_curve.npy\"), np.array([avg_losses, val_losses]))\n",
    "\n",
    "    # 学習終了後、モデルを保存\n",
    "    torch.save(model.state_dict(), os.path.join(model_path, \"final_model.pt\"))\n",
    "\n",
    "    # 学習曲線を出力\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(range(1, epochs+1), avg_losses, label='Train Loss')\n",
    "    plt.plot(range(1, epochs+1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 学習データのファイルパス\n",
    "data_file = \"../Datasets/archive/reshaped_text_embeds.npy\"\n",
    "# モデルの保存先ディレクトリ\n",
    "model_path = \"../Models/ClIP_converter/\"\n",
    "\n",
    "# ディレクトリが存在しない場合は作成\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# CUDAが利用可能かチェック\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# モデルの初期化\n",
    "model = TransformerModel().to(device)\n",
    "if device == \"cuda\":\n",
    "    model = model.cuda()\n",
    "\n",
    "# データの前処理と分割\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = preprocess_data(data_file)\n",
    "\n",
    "# ハイパーパラメータの最適化\n",
    "learning_rate, criterion, optimizer = optimize_hyperparameters(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# モデルの学習\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "train_model(model, criterion, optimizer, X_train, y_train, X_val, y_val, X_test, y_test, epochs, batch_size, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e86fb6-bb3b-42b0-8ff3-0569e8a22507",
   "metadata": {},
   "source": [
    "**Model Load Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81045825-0d8a-40f7-99ce-effe8dcd09bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([1, 512])\n",
      "Sample of output tensor: tensor([[-0.0407,  0.0420, -0.0433,  0.0037, -0.0062, -0.0019,  0.0267,  0.0348,\n",
      "          0.0090, -0.0205,  0.0404, -0.0121, -0.0046, -0.0036,  0.0285,  0.0110,\n",
      "         -0.0206,  0.0065,  0.0068, -0.0349, -0.0176,  0.0030,  0.0074,  0.0229,\n",
      "         -0.0210,  0.0029, -0.0183, -0.0021,  0.0298, -0.0150, -0.0274,  0.0070,\n",
      "          0.0160, -0.0021,  0.0084,  0.0241, -0.0079, -0.0246,  0.0521,  0.0162,\n",
      "          0.0813,  0.0542,  0.0023, -0.0403,  0.0340, -0.0165, -0.0344, -0.0055,\n",
      "          0.0346, -0.0241, -0.0031,  0.0134,  0.0007,  0.0206, -0.0029,  0.0435,\n",
      "         -0.0192,  0.0019,  0.0210, -0.0099, -0.0242,  0.0012,  0.0096,  0.0271,\n",
      "         -0.0080, -0.0056,  0.0030,  0.0211, -0.0122,  0.0196, -0.0419, -0.0130,\n",
      "          0.0002, -0.0432, -0.0140, -0.0406,  0.0286, -0.0083, -0.0769, -0.0689,\n",
      "          0.0386, -0.0707, -0.0088, -0.0280, -0.0173,  0.0089,  0.0195,  0.0104,\n",
      "          0.0401,  0.0102, -0.0321, -0.0255, -0.0439, -0.0119,  0.0029,  0.0208,\n",
      "         -0.0566, -0.0169, -0.0047,  0.0311, -0.0135,  0.0240,  0.0201,  0.0351,\n",
      "          0.0248, -0.0340,  0.0106, -0.0089, -0.0129, -0.0078, -0.0075, -0.0827,\n",
      "         -0.0238,  0.0021,  0.0362, -0.0138,  0.0286, -0.0069,  0.0081,  0.0171,\n",
      "          0.0105, -0.0358, -0.0417, -0.0335,  0.0445,  0.0413,  0.0196, -0.0162,\n",
      "         -0.0091, -0.0234,  0.0221, -0.0124, -0.0243,  0.0369,  0.0395, -0.0234,\n",
      "          0.0460,  0.0213,  0.0684, -0.0154,  0.0551, -0.0384, -0.0328,  0.0267,\n",
      "         -0.0060, -0.0036,  0.0133, -0.0266,  0.0127,  0.0153, -0.0386, -0.0273,\n",
      "          0.0348,  0.0466, -0.0178, -0.0323, -0.0157, -0.0490, -0.0066,  0.0200,\n",
      "          0.0130, -0.0502,  0.0095, -0.0023,  0.0100, -0.0337,  0.0422, -0.0237,\n",
      "          0.0082,  0.0288,  0.0503,  0.0491,  0.0407,  0.0031, -0.0393, -0.0223,\n",
      "          0.0408, -0.0140, -0.0072, -0.0196, -0.0226, -0.0754, -0.0165,  0.0195,\n",
      "          0.0186,  0.0391, -0.0220,  0.0163,  0.0221,  0.0019,  0.0134, -0.0193,\n",
      "          0.0431,  0.0364,  0.0213,  0.0077,  0.0123,  0.0340,  0.0170,  0.0290,\n",
      "          0.0095,  0.0285,  0.0004,  0.0046,  0.0326,  0.0117,  0.0289,  0.0539,\n",
      "         -0.0206, -0.0256, -0.0520,  0.0278,  0.0191, -0.0223,  0.0280,  0.0039,\n",
      "         -0.0126, -0.0413, -0.0534, -0.0251,  0.0125,  0.0103, -0.0511, -0.0240,\n",
      "         -0.0687,  0.0017,  0.0326,  0.0112,  0.0289,  0.0693, -0.0064,  0.0047,\n",
      "          0.0192, -0.0431, -0.0245,  0.0436, -0.0022, -0.0150, -0.0397,  0.0510,\n",
      "          0.0306, -0.0178,  0.0070,  0.0093, -0.0085, -0.0288,  0.0140, -0.0003,\n",
      "          0.0590,  0.0289, -0.0285, -0.0202,  0.0350,  0.0181,  0.0293,  0.0200,\n",
      "         -0.0660,  0.0280,  0.0342, -0.0042, -0.0356, -0.0334,  0.0097, -0.0631,\n",
      "          0.0512, -0.0415, -0.0105,  0.0239,  0.0041, -0.0398,  0.0170,  0.0154,\n",
      "         -0.0150, -0.0393, -0.0128, -0.0184, -0.0666,  0.0356,  0.0127, -0.0307,\n",
      "         -0.0170,  0.0107, -0.0101, -0.0238,  0.0653,  0.0227, -0.0039, -0.0114,\n",
      "         -0.0095, -0.0510,  0.0156,  0.0638, -0.0073, -0.0546,  0.0228, -0.0009,\n",
      "         -0.0039,  0.0211, -0.0349,  0.0274,  0.0478, -0.0215,  0.0074,  0.0338,\n",
      "         -0.0067, -0.0423,  0.0239,  0.0112, -0.0032, -0.0575,  0.0203, -0.0255,\n",
      "          0.0159, -0.0147, -0.0186,  0.0005, -0.0189, -0.0190, -0.0078,  0.0009,\n",
      "         -0.0152, -0.0411, -0.0152,  0.0278,  0.0130, -0.0515, -0.0449, -0.0043,\n",
      "          0.0257,  0.0191,  0.0184, -0.0417,  0.0121, -0.0494, -0.0062, -0.0479,\n",
      "          0.0177,  0.0101, -0.0117,  0.0453, -0.0508, -0.0385, -0.0228, -0.0016,\n",
      "          0.0432, -0.0163,  0.0048, -0.0256,  0.0378, -0.0041,  0.0011,  0.0380,\n",
      "         -0.0179,  0.0339,  0.0295,  0.0238,  0.0560, -0.0252, -0.0535,  0.0531,\n",
      "         -0.0069, -0.0232, -0.0370,  0.0297,  0.0263, -0.0242,  0.0259, -0.0241,\n",
      "          0.0019,  0.0089,  0.0471,  0.0058,  0.0260, -0.0064,  0.0004,  0.0132,\n",
      "         -0.0017, -0.0295, -0.0078, -0.0362, -0.0309,  0.0160,  0.0106,  0.0185,\n",
      "         -0.0452,  0.0033, -0.0171,  0.0339, -0.0070,  0.0026,  0.0216,  0.0379,\n",
      "          0.0088, -0.0405,  0.0272,  0.0008,  0.0511,  0.0283, -0.0096, -0.0244,\n",
      "         -0.0023,  0.0086,  0.0576,  0.0329, -0.0148,  0.0338, -0.0099,  0.0157,\n",
      "         -0.0431,  0.0016,  0.0137, -0.0729,  0.0192,  0.0097, -0.0104, -0.0160,\n",
      "          0.0128,  0.0389,  0.0027,  0.0122, -0.0388, -0.0128, -0.0115, -0.0422,\n",
      "         -0.0125, -0.0001,  0.0181, -0.0147,  0.0082,  0.0212, -0.0357, -0.0155,\n",
      "          0.0253, -0.0487, -0.0029,  0.0148,  0.0362,  0.0521, -0.0403, -0.0489,\n",
      "         -0.0369, -0.0063,  0.0453, -0.0178,  0.0335, -0.0225, -0.0165, -0.0386,\n",
      "          0.0514, -0.0246,  0.0262,  0.0134,  0.0034, -0.0507,  0.0196, -0.0208,\n",
      "         -0.0364,  0.0031, -0.0158,  0.0020, -0.0047, -0.0584, -0.0140, -0.0266,\n",
      "          0.0086,  0.0265, -0.0149,  0.0207,  0.0111,  0.0447, -0.0138, -0.0141,\n",
      "          0.0139,  0.0062,  0.0158, -0.0348, -0.0338,  0.0118, -0.0306, -0.0280,\n",
      "         -0.0497, -0.0051,  0.0167, -0.0055, -0.0283,  0.0245, -0.0109, -0.0030,\n",
      "         -0.0445, -0.0122, -0.0166,  0.0053, -0.0176,  0.0195, -0.0589, -0.0294,\n",
      "          0.0056, -0.0108, -0.0095, -0.0054, -0.0120, -0.0462, -0.0362, -0.0430,\n",
      "          0.0149,  0.0045,  0.0048,  0.0309,  0.0517, -0.0219, -0.0083,  0.0338]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "\n",
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=6)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        memory = self.encoder(src)\n",
    "        output = self.decoder(tgt, memory)\n",
    "        return output\n",
    "\n",
    "# 学習後に保存されたモデルのパス\n",
    "model_path = \"../Models/ClIP_converter/best_model.pt\"\n",
    "\n",
    "# モデルの初期化\n",
    "model = TransformerModel()\n",
    "# GPUを利用可能であればGPUにモデルを転送\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# テスト用のランダムな(1,512)のテンソルを生成\n",
    "random_input = torch.randn(1, 512).to(device)\n",
    "\n",
    "# モデルにランダムな入力を与えてテスト\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(random_input, random_input)\n",
    "\n",
    "# 出力を表示\n",
    "print(\"Output tensor shape:\", output.shape)\n",
    "print(\"Sample of output tensor:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a5e2e-80f4-41ec-ba02-342f342658a2",
   "metadata": {},
   "source": [
    "**CLIP+Converter vs CLAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa98753e-7e19-4a15-b45d-028a99e5d9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "tokenizer_config.json: 100%|██████████████████████████████████████████████████████████████████████████████| 28.0/28.0 [00:00<00:00, 77.9kB/s]\n",
      "vocab.txt: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 553kB/s]\n",
      "tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 935kB/s]\n",
      "config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 570/570 [00:00<00:00, 1.89MB/s]\n",
      "vocab.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 1.85MB/s]\n",
      "merges.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 2.20MB/s]\n",
      "tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 18.3MB/s]\n",
      "config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 481/481 [00:00<00:00, 1.30MB/s]\n",
      "vocab.json: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 899k/899k [00:00<00:00, 24.9MB/s]\n",
      "merges.txt: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 4.37MB/s]\n",
      "tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████| 1.36M/1.36M [00:00<00:00, 51.5MB/s]\n",
      "config.json: 100%|██████████████████████████████████████████████████████████████████████████████████████| 1.72k/1.72k [00:00<00:00, 4.57MB/s]\n",
      "100%|███████████████████████████████████████| 338M/338M [00:19<00:00, 18.3MiB/s]\n",
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201336/work/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "model.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████| 499M/499M [00:09<00:00, 51.5MB/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the specified checkpoint ../Models/Laion_CLAP/music_audioset_epoch_15_esc_90.14.patched.pt from users.\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "import laion_clap\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "model1, preprocess1 = clip.load(\"ViT-B/32\", device=device)\n",
    "model2 = laion_clap.CLAP_Module(enable_fusion=False, amodel= 'HTSAT-base')\n",
    "model2.load_ckpt('../Models/Laion_CLAP/music_audioset_epoch_15_esc_90.14.patched.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54d6640a-109f-4c41-a4d7-4f42b83d6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.float16\n",
      "torch.float32\n",
      "torch.Size([1, 512])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "text_data = \"a cat\"\n",
    "with torch.no_grad():\n",
    "    # model2.get_text_embeddingの前にtext_dataを配列に変換\n",
    "    token = clip.tokenize(text_data).to(device)\n",
    "    text_embed_clip = model1.encode_text(token)\n",
    "    print(text_embed_clip.shape)\n",
    "    print(text_embed_clip.dtype)\n",
    "    text_embed_clip = text_embed_clip.to(torch.float32)\n",
    "    print(text_embed_clip.dtype)\n",
    "\n",
    "    # text_dataを2要素の配列に変換\n",
    "    text_data_array = [text_data, \"\"]\n",
    "    text_embed_clap = model2.get_text_embedding(text_data_array, use_tensor=True).to(device)\n",
    "    # [2, 512]のTensorから第一要素を取り出し、[1, 512]のTensorにする\n",
    "    text_embed_clap = text_embed_clap[0].unsqueeze(0)\n",
    "    print(text_embed_clap.shape)\n",
    "    print(text_embed_clap.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "196cd3ca-900f-475a-8e81-cba770be4938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tensor shape: torch.Size([1, 512])\n",
      "Output tensor dtype: torch.float32\n",
      "tensor(0.0020, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# モデルにランダムな入力を与えてテスト\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    output = model(text_embed_clip, text_embed_clip)\n",
    "\n",
    "# 出力を表示\n",
    "print(\"Output tensor shape:\", output.shape)\n",
    "print(\"Output tensor dtype:\", output.dtype)\n",
    "#print(\"Sample of output tensor:\", output)\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, text_embed_clap)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "892599d6-b383-4d43-b6a7-0bfa0722dd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02b760-4f40-44aa-b0e7-972d8354706e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b2676-f8ab-4a60-ad41-0cd03f10fe68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd94f6-68a8-4157-865b-0abd4c5fb579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
